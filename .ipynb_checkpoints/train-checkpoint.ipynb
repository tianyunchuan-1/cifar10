{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_train 50000\n",
      "num_of_test 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from vggnet import VGGNet\n",
    "from load_cifar10 import train_loader, test_loader\n",
    "import os\n",
    "import tensorboardX\n",
    "from resnet import resnet\n",
    "from mobilenetv1 import mobilenetv1_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/tianyunchuan/iCloudDrive/_data_/data_log/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import _path\n",
    "PATH_HEAD_DATA = _path.PATH_HEAD\n",
    "PATH_LOG = r'{}_data_/data_log/'.format(PATH_HEAD_DATA)\n",
    "PATH_LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('{}model'.format(PATH_LOG)):\n",
    "    os.mkdir('{}model'.format(PATH_LOG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mobilenet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv_dw2): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (conv_dw3): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (conv_dw4): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (conv_dw5): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (conv_dw6): Sequential(\n",
       "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (conv_dw7): Sequential(\n",
       "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (conv_dw8): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (conv_dw9): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "batch_size = 100\n",
    "lr = 0.01\n",
    "# net = VGGNet().to(device)\n",
    "# net = resnet().to(device)\n",
    "net = mobilenetv1_small().to(device)\n",
    "net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# lr\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is:  0\n",
      "epoch:  0 train step:  0 loss:  2.3534767627716064 mini-batch correct:  14.0 train lr: 0.01\n",
      "epoch:  0 train step:  1 loss:  3.1685826778411865 mini-batch correct:  21.0 train lr: 0.01\n",
      "epoch:  0 train step:  2 loss:  2.7068512439727783 mini-batch correct:  11.0 train lr: 0.01\n",
      "epoch:  0 train step:  3 loss:  2.508357524871826 mini-batch correct:  16.0 train lr: 0.01\n",
      "epoch:  0 train step:  4 loss:  2.6644082069396973 mini-batch correct:  18.0 train lr: 0.01\n",
      "epoch:  0 train step:  5 loss:  2.4549975395202637 mini-batch correct:  27.0 train lr: 0.01\n",
      "epoch:  0 train step:  6 loss:  2.414590358734131 mini-batch correct:  17.0 train lr: 0.01\n",
      "epoch:  0 train step:  7 loss:  2.3078486919403076 mini-batch correct:  17.0 train lr: 0.01\n",
      "epoch:  0 train step:  8 loss:  2.221675157546997 mini-batch correct:  14.0 train lr: 0.01\n",
      "epoch:  0 train step:  9 loss:  2.132326364517212 mini-batch correct:  26.0 train lr: 0.01\n",
      "epoch:  0 train step:  10 loss:  2.1446456909179688 mini-batch correct:  26.0 train lr: 0.01\n",
      "epoch:  0 train step:  11 loss:  2.1738204956054688 mini-batch correct:  30.0 train lr: 0.01\n",
      "epoch:  0 train step:  12 loss:  2.276772975921631 mini-batch correct:  16.0 train lr: 0.01\n",
      "epoch:  0 train step:  13 loss:  2.282149314880371 mini-batch correct:  22.0 train lr: 0.01\n",
      "epoch:  0 train step:  14 loss:  2.1671433448791504 mini-batch correct:  22.0 train lr: 0.01\n",
      "epoch:  0 train step:  15 loss:  2.2591114044189453 mini-batch correct:  21.0 train lr: 0.01\n",
      "epoch:  0 train step:  16 loss:  2.170426368713379 mini-batch correct:  21.0 train lr: 0.01\n",
      "epoch:  0 train step:  17 loss:  2.202451467514038 mini-batch correct:  22.0 train lr: 0.01\n",
      "epoch:  0 train step:  18 loss:  2.0995383262634277 mini-batch correct:  23.0 train lr: 0.01\n",
      "epoch:  0 train step:  19 loss:  2.1665260791778564 mini-batch correct:  25.0 train lr: 0.01\n",
      "epoch:  0 train step:  20 loss:  2.1783194541931152 mini-batch correct:  26.0 train lr: 0.01\n",
      "epoch:  0 train step:  21 loss:  2.168031930923462 mini-batch correct:  20.0 train lr: 0.01\n",
      "epoch:  0 train step:  22 loss:  2.064049243927002 mini-batch correct:  20.0 train lr: 0.01\n",
      "epoch:  0 train step:  23 loss:  2.1922221183776855 mini-batch correct:  20.0 train lr: 0.01\n",
      "epoch:  0 train step:  24 loss:  2.0833327770233154 mini-batch correct:  30.0 train lr: 0.01\n",
      "epoch:  0 train step:  25 loss:  2.0860581398010254 mini-batch correct:  25.0 train lr: 0.01\n",
      "epoch:  0 train step:  26 loss:  1.9447593688964844 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  27 loss:  2.2333316802978516 mini-batch correct:  23.0 train lr: 0.01\n",
      "epoch:  0 train step:  28 loss:  2.1639404296875 mini-batch correct:  22.0 train lr: 0.01\n",
      "epoch:  0 train step:  29 loss:  2.125356674194336 mini-batch correct:  27.0 train lr: 0.01\n",
      "epoch:  0 train step:  30 loss:  2.0872604846954346 mini-batch correct:  34.0 train lr: 0.01\n",
      "epoch:  0 train step:  31 loss:  2.058250665664673 mini-batch correct:  28.0 train lr: 0.01\n",
      "epoch:  0 train step:  32 loss:  2.138601779937744 mini-batch correct:  27.0 train lr: 0.01\n",
      "epoch:  0 train step:  33 loss:  2.133087396621704 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  34 loss:  2.1539034843444824 mini-batch correct:  17.0 train lr: 0.01\n",
      "epoch:  0 train step:  35 loss:  2.07326602935791 mini-batch correct:  26.0 train lr: 0.01\n",
      "epoch:  0 train step:  36 loss:  2.128859519958496 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  37 loss:  2.212984561920166 mini-batch correct:  23.0 train lr: 0.01\n",
      "epoch:  0 train step:  38 loss:  2.2328994274139404 mini-batch correct:  20.0 train lr: 0.01\n",
      "epoch:  0 train step:  39 loss:  2.05515456199646 mini-batch correct:  19.0 train lr: 0.01\n",
      "epoch:  0 train step:  40 loss:  2.0702433586120605 mini-batch correct:  28.0 train lr: 0.01\n",
      "epoch:  0 train step:  41 loss:  2.1598260402679443 mini-batch correct:  18.0 train lr: 0.01\n",
      "epoch:  0 train step:  42 loss:  2.073625087738037 mini-batch correct:  29.0 train lr: 0.01\n",
      "epoch:  0 train step:  43 loss:  2.1487951278686523 mini-batch correct:  29.0 train lr: 0.01\n",
      "epoch:  0 train step:  44 loss:  1.9835689067840576 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  45 loss:  2.0781548023223877 mini-batch correct:  25.0 train lr: 0.01\n",
      "epoch:  0 train step:  46 loss:  2.0058062076568604 mini-batch correct:  30.0 train lr: 0.01\n",
      "epoch:  0 train step:  47 loss:  2.0281412601470947 mini-batch correct:  30.0 train lr: 0.01\n",
      "epoch:  0 train step:  48 loss:  1.9527534246444702 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  49 loss:  2.0596818923950195 mini-batch correct:  30.0 train lr: 0.01\n",
      "epoch:  0 train step:  50 loss:  2.0212595462799072 mini-batch correct:  26.0 train lr: 0.01\n",
      "epoch:  0 train step:  51 loss:  2.034970760345459 mini-batch correct:  28.0 train lr: 0.01\n",
      "epoch:  0 train step:  52 loss:  1.9248837232589722 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  53 loss:  1.9905794858932495 mini-batch correct:  28.0 train lr: 0.01\n",
      "epoch:  0 train step:  54 loss:  1.9851962327957153 mini-batch correct:  29.0 train lr: 0.01\n",
      "epoch:  0 train step:  55 loss:  1.997461199760437 mini-batch correct:  22.0 train lr: 0.01\n",
      "epoch:  0 train step:  56 loss:  1.991196870803833 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  57 loss:  2.045522451400757 mini-batch correct:  25.0 train lr: 0.01\n",
      "epoch:  0 train step:  58 loss:  2.025675058364868 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  59 loss:  2.0107736587524414 mini-batch correct:  26.0 train lr: 0.01\n",
      "epoch:  0 train step:  60 loss:  1.9335157871246338 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  61 loss:  2.0581674575805664 mini-batch correct:  34.0 train lr: 0.01\n",
      "epoch:  0 train step:  62 loss:  2.0936384201049805 mini-batch correct:  32.0 train lr: 0.01\n",
      "epoch:  0 train step:  63 loss:  2.013169527053833 mini-batch correct:  26.0 train lr: 0.01\n",
      "epoch:  0 train step:  64 loss:  2.0057334899902344 mini-batch correct:  27.0 train lr: 0.01\n",
      "epoch:  0 train step:  65 loss:  1.924160122871399 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  66 loss:  2.033191442489624 mini-batch correct:  36.0 train lr: 0.01\n",
      "epoch:  0 train step:  67 loss:  1.9877088069915771 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  68 loss:  1.9580731391906738 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  69 loss:  2.0099875926971436 mini-batch correct:  24.0 train lr: 0.01\n",
      "epoch:  0 train step:  70 loss:  1.8911869525909424 mini-batch correct:  39.0 train lr: 0.01\n",
      "epoch:  0 train step:  71 loss:  2.0276286602020264 mini-batch correct:  25.0 train lr: 0.01\n",
      "epoch:  0 train step:  72 loss:  2.0718984603881836 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  73 loss:  2.114902973175049 mini-batch correct:  29.0 train lr: 0.01\n",
      "epoch:  0 train step:  74 loss:  2.0409324169158936 mini-batch correct:  29.0 train lr: 0.01\n",
      "epoch:  0 train step:  75 loss:  2.000089645385742 mini-batch correct:  32.0 train lr: 0.01\n",
      "epoch:  0 train step:  76 loss:  2.0415892601013184 mini-batch correct:  30.0 train lr: 0.01\n",
      "epoch:  0 train step:  77 loss:  1.8738842010498047 mini-batch correct:  43.0 train lr: 0.01\n",
      "epoch:  0 train step:  78 loss:  2.07096266746521 mini-batch correct:  29.0 train lr: 0.01\n",
      "epoch:  0 train step:  79 loss:  2.0694594383239746 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  80 loss:  1.9587123394012451 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  81 loss:  1.979820966720581 mini-batch correct:  32.0 train lr: 0.01\n",
      "epoch:  0 train step:  82 loss:  2.063689708709717 mini-batch correct:  25.0 train lr: 0.01\n",
      "epoch:  0 train step:  83 loss:  2.04117488861084 mini-batch correct:  28.0 train lr: 0.01\n",
      "epoch:  0 train step:  84 loss:  2.093580961227417 mini-batch correct:  29.0 train lr: 0.01\n",
      "epoch:  0 train step:  85 loss:  2.001572608947754 mini-batch correct:  28.0 train lr: 0.01\n",
      "epoch:  0 train step:  86 loss:  2.1784234046936035 mini-batch correct:  27.0 train lr: 0.01\n",
      "epoch:  0 train step:  87 loss:  1.9785826206207275 mini-batch correct:  32.0 train lr: 0.01\n",
      "epoch:  0 train step:  88 loss:  1.890602707862854 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  89 loss:  1.9941213130950928 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  90 loss:  2.0676865577697754 mini-batch correct:  24.0 train lr: 0.01\n",
      "epoch:  0 train step:  91 loss:  2.009310722351074 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  92 loss:  2.001887559890747 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  93 loss:  2.0310397148132324 mini-batch correct:  30.0 train lr: 0.01\n",
      "epoch:  0 train step:  94 loss:  2.0358731746673584 mini-batch correct:  26.0 train lr: 0.01\n",
      "epoch:  0 train step:  95 loss:  1.9751863479614258 mini-batch correct:  28.0 train lr: 0.01\n",
      "epoch:  0 train step:  96 loss:  1.9210058450698853 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  97 loss:  1.8850804567337036 mini-batch correct:  32.0 train lr: 0.01\n",
      "epoch:  0 train step:  98 loss:  2.02620530128479 mini-batch correct:  36.0 train lr: 0.01\n",
      "epoch:  0 train step:  99 loss:  1.9975509643554688 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  100 loss:  2.0197720527648926 mini-batch correct:  29.0 train lr: 0.01\n",
      "epoch:  0 train step:  101 loss:  2.004948616027832 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  102 loss:  1.9445664882659912 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  103 loss:  1.9766730070114136 mini-batch correct:  34.0 train lr: 0.01\n",
      "epoch:  0 train step:  104 loss:  1.8795655965805054 mini-batch correct:  32.0 train lr: 0.01\n",
      "epoch:  0 train step:  105 loss:  1.870681881904602 mini-batch correct:  34.0 train lr: 0.01\n",
      "epoch:  0 train step:  106 loss:  1.8381730318069458 mini-batch correct:  44.0 train lr: 0.01\n",
      "epoch:  0 train step:  107 loss:  1.8412545919418335 mini-batch correct:  40.0 train lr: 0.01\n",
      "epoch:  0 train step:  108 loss:  1.985341191291809 mini-batch correct:  32.0 train lr: 0.01\n",
      "epoch:  0 train step:  109 loss:  1.9655539989471436 mini-batch correct:  23.0 train lr: 0.01\n",
      "epoch:  0 train step:  110 loss:  1.9835633039474487 mini-batch correct:  30.0 train lr: 0.01\n",
      "epoch:  0 train step:  111 loss:  2.111327886581421 mini-batch correct:  32.0 train lr: 0.01\n",
      "epoch:  0 train step:  112 loss:  1.8310614824295044 mini-batch correct:  41.0 train lr: 0.01\n",
      "epoch:  0 train step:  113 loss:  2.0087153911590576 mini-batch correct:  28.0 train lr: 0.01\n",
      "epoch:  0 train step:  114 loss:  2.0317721366882324 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  115 loss:  2.113276958465576 mini-batch correct:  19.0 train lr: 0.01\n",
      "epoch:  0 train step:  116 loss:  1.8775960206985474 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  117 loss:  1.978105902671814 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  118 loss:  1.933293104171753 mini-batch correct:  30.0 train lr: 0.01\n",
      "epoch:  0 train step:  119 loss:  2.028343677520752 mini-batch correct:  26.0 train lr: 0.01\n",
      "epoch:  0 train step:  120 loss:  1.9489843845367432 mini-batch correct:  29.0 train lr: 0.01\n",
      "epoch:  0 train step:  121 loss:  1.9851572513580322 mini-batch correct:  27.0 train lr: 0.01\n",
      "epoch:  0 train step:  122 loss:  1.9163596630096436 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  123 loss:  2.087522506713867 mini-batch correct:  27.0 train lr: 0.01\n",
      "epoch:  0 train step:  124 loss:  1.9977854490280151 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  125 loss:  2.0961973667144775 mini-batch correct:  24.0 train lr: 0.01\n",
      "epoch:  0 train step:  126 loss:  2.113027572631836 mini-batch correct:  23.0 train lr: 0.01\n",
      "epoch:  0 train step:  127 loss:  1.9709110260009766 mini-batch correct:  34.0 train lr: 0.01\n",
      "epoch:  0 train step:  128 loss:  1.9884403944015503 mini-batch correct:  34.0 train lr: 0.01\n",
      "epoch:  0 train step:  129 loss:  1.9148225784301758 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  130 loss:  1.98823082447052 mini-batch correct:  30.0 train lr: 0.01\n",
      "epoch:  0 train step:  131 loss:  1.9103772640228271 mini-batch correct:  41.0 train lr: 0.01\n",
      "epoch:  0 train step:  132 loss:  2.0792055130004883 mini-batch correct:  29.0 train lr: 0.01\n",
      "epoch:  0 train step:  133 loss:  1.9318147897720337 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  134 loss:  1.973101019859314 mini-batch correct:  39.0 train lr: 0.01\n",
      "epoch:  0 train step:  135 loss:  1.8658113479614258 mini-batch correct:  41.0 train lr: 0.01\n",
      "epoch:  0 train step:  136 loss:  1.8720810413360596 mini-batch correct:  36.0 train lr: 0.01\n",
      "epoch:  0 train step:  137 loss:  1.8853931427001953 mini-batch correct:  36.0 train lr: 0.01\n",
      "epoch:  0 train step:  138 loss:  1.98251473903656 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  139 loss:  2.0450599193573 mini-batch correct:  29.0 train lr: 0.01\n",
      "epoch:  0 train step:  140 loss:  1.9492733478546143 mini-batch correct:  36.0 train lr: 0.01\n",
      "epoch:  0 train step:  141 loss:  1.8881276845932007 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  142 loss:  1.9127998352050781 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  143 loss:  2.026383638381958 mini-batch correct:  27.0 train lr: 0.01\n",
      "epoch:  0 train step:  144 loss:  1.9711217880249023 mini-batch correct:  29.0 train lr: 0.01\n",
      "epoch:  0 train step:  145 loss:  2.057511568069458 mini-batch correct:  26.0 train lr: 0.01\n",
      "epoch:  0 train step:  146 loss:  1.905452847480774 mini-batch correct:  30.0 train lr: 0.01\n",
      "epoch:  0 train step:  147 loss:  2.029118061065674 mini-batch correct:  27.0 train lr: 0.01\n",
      "epoch:  0 train step:  148 loss:  2.036151647567749 mini-batch correct:  28.0 train lr: 0.01\n",
      "epoch:  0 train step:  149 loss:  1.9484410285949707 mini-batch correct:  39.0 train lr: 0.01\n",
      "epoch:  0 train step:  150 loss:  2.1618165969848633 mini-batch correct:  29.0 train lr: 0.01\n",
      "epoch:  0 train step:  151 loss:  1.9457428455352783 mini-batch correct:  32.0 train lr: 0.01\n",
      "epoch:  0 train step:  152 loss:  1.9387091398239136 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  153 loss:  1.9686484336853027 mini-batch correct:  28.0 train lr: 0.01\n",
      "epoch:  0 train step:  154 loss:  1.8910765647888184 mini-batch correct:  45.0 train lr: 0.01\n",
      "epoch:  0 train step:  155 loss:  1.8719542026519775 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  156 loss:  1.8597095012664795 mini-batch correct:  41.0 train lr: 0.01\n",
      "epoch:  0 train step:  157 loss:  2.07704496383667 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  158 loss:  1.848458170890808 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  159 loss:  1.8127787113189697 mini-batch correct:  45.0 train lr: 0.01\n",
      "epoch:  0 train step:  160 loss:  1.9683500528335571 mini-batch correct:  34.0 train lr: 0.01\n",
      "epoch:  0 train step:  161 loss:  2.073984384536743 mini-batch correct:  26.0 train lr: 0.01\n",
      "epoch:  0 train step:  162 loss:  1.994659185409546 mini-batch correct:  28.0 train lr: 0.01\n",
      "epoch:  0 train step:  163 loss:  1.974563479423523 mini-batch correct:  40.0 train lr: 0.01\n",
      "epoch:  0 train step:  164 loss:  1.9895415306091309 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  165 loss:  1.9013347625732422 mini-batch correct:  32.0 train lr: 0.01\n",
      "epoch:  0 train step:  166 loss:  1.9733960628509521 mini-batch correct:  30.0 train lr: 0.01\n",
      "epoch:  0 train step:  167 loss:  1.8799736499786377 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  168 loss:  1.8609402179718018 mini-batch correct:  41.0 train lr: 0.01\n",
      "epoch:  0 train step:  169 loss:  1.918894648551941 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  170 loss:  1.9439929723739624 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  171 loss:  2.004739284515381 mini-batch correct:  34.0 train lr: 0.01\n",
      "epoch:  0 train step:  172 loss:  1.8994944095611572 mini-batch correct:  40.0 train lr: 0.01\n",
      "epoch:  0 train step:  173 loss:  2.004286289215088 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  174 loss:  1.941325068473816 mini-batch correct:  29.0 train lr: 0.01\n",
      "epoch:  0 train step:  175 loss:  1.8834455013275146 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  176 loss:  1.9248745441436768 mini-batch correct:  40.0 train lr: 0.01\n",
      "epoch:  0 train step:  177 loss:  1.9354811906814575 mini-batch correct:  28.0 train lr: 0.01\n",
      "epoch:  0 train step:  178 loss:  1.8778152465820312 mini-batch correct:  41.0 train lr: 0.01\n",
      "epoch:  0 train step:  179 loss:  1.9298259019851685 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  180 loss:  1.9437205791473389 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  181 loss:  1.8497366905212402 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  182 loss:  1.9069099426269531 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  183 loss:  2.0015430450439453 mini-batch correct:  32.0 train lr: 0.01\n",
      "epoch:  0 train step:  184 loss:  1.987143874168396 mini-batch correct:  26.0 train lr: 0.01\n",
      "epoch:  0 train step:  185 loss:  1.9184411764144897 mini-batch correct:  36.0 train lr: 0.01\n",
      "epoch:  0 train step:  186 loss:  1.9049932956695557 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  187 loss:  1.8491697311401367 mini-batch correct:  32.0 train lr: 0.01\n",
      "epoch:  0 train step:  188 loss:  1.9923362731933594 mini-batch correct:  29.0 train lr: 0.01\n",
      "epoch:  0 train step:  189 loss:  1.9191049337387085 mini-batch correct:  40.0 train lr: 0.01\n",
      "epoch:  0 train step:  190 loss:  1.968139886856079 mini-batch correct:  32.0 train lr: 0.01\n",
      "epoch:  0 train step:  191 loss:  1.8625208139419556 mini-batch correct:  39.0 train lr: 0.01\n",
      "epoch:  0 train step:  192 loss:  1.9136826992034912 mini-batch correct:  34.0 train lr: 0.01\n",
      "epoch:  0 train step:  193 loss:  1.8479138612747192 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  194 loss:  1.956903338432312 mini-batch correct:  36.0 train lr: 0.01\n",
      "epoch:  0 train step:  195 loss:  2.055995464324951 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  196 loss:  1.9880093336105347 mini-batch correct:  32.0 train lr: 0.01\n",
      "epoch:  0 train step:  197 loss:  1.9070806503295898 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  198 loss:  2.0517520904541016 mini-batch correct:  22.0 train lr: 0.01\n",
      "epoch:  0 train step:  199 loss:  1.7981462478637695 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  200 loss:  1.9398823976516724 mini-batch correct:  41.0 train lr: 0.01\n",
      "epoch:  0 train step:  201 loss:  2.0500986576080322 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  202 loss:  1.9302854537963867 mini-batch correct:  36.0 train lr: 0.01\n",
      "epoch:  0 train step:  203 loss:  1.821611762046814 mini-batch correct:  40.0 train lr: 0.01\n",
      "epoch:  0 train step:  204 loss:  1.9492956399917603 mini-batch correct:  32.0 train lr: 0.01\n",
      "epoch:  0 train step:  205 loss:  1.8090169429779053 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  206 loss:  1.834082841873169 mini-batch correct:  40.0 train lr: 0.01\n",
      "epoch:  0 train step:  207 loss:  1.9916985034942627 mini-batch correct:  36.0 train lr: 0.01\n",
      "epoch:  0 train step:  208 loss:  1.9723684787750244 mini-batch correct:  32.0 train lr: 0.01\n",
      "epoch:  0 train step:  209 loss:  1.807332992553711 mini-batch correct:  43.0 train lr: 0.01\n",
      "epoch:  0 train step:  210 loss:  1.942954421043396 mini-batch correct:  41.0 train lr: 0.01\n",
      "epoch:  0 train step:  211 loss:  1.9497889280319214 mini-batch correct:  39.0 train lr: 0.01\n",
      "epoch:  0 train step:  212 loss:  2.0222175121307373 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  213 loss:  2.0136733055114746 mini-batch correct:  27.0 train lr: 0.01\n",
      "epoch:  0 train step:  214 loss:  1.8483355045318604 mini-batch correct:  41.0 train lr: 0.01\n",
      "epoch:  0 train step:  215 loss:  2.035871982574463 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  216 loss:  1.9047400951385498 mini-batch correct:  39.0 train lr: 0.01\n",
      "epoch:  0 train step:  217 loss:  1.8687472343444824 mini-batch correct:  39.0 train lr: 0.01\n",
      "epoch:  0 train step:  218 loss:  1.864700198173523 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  219 loss:  1.9623873233795166 mini-batch correct:  34.0 train lr: 0.01\n",
      "epoch:  0 train step:  220 loss:  1.829702615737915 mini-batch correct:  50.0 train lr: 0.01\n",
      "epoch:  0 train step:  221 loss:  1.9361248016357422 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  222 loss:  1.9778860807418823 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  223 loss:  1.9487966299057007 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  224 loss:  1.9458650350570679 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  225 loss:  1.912243127822876 mini-batch correct:  36.0 train lr: 0.01\n",
      "epoch:  0 train step:  226 loss:  1.8630247116088867 mini-batch correct:  40.0 train lr: 0.01\n",
      "epoch:  0 train step:  227 loss:  1.8419119119644165 mini-batch correct:  45.0 train lr: 0.01\n",
      "epoch:  0 train step:  228 loss:  1.8781747817993164 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  229 loss:  1.9374208450317383 mini-batch correct:  36.0 train lr: 0.01\n",
      "epoch:  0 train step:  230 loss:  1.8397043943405151 mini-batch correct:  43.0 train lr: 0.01\n",
      "epoch:  0 train step:  231 loss:  2.057610034942627 mini-batch correct:  27.0 train lr: 0.01\n",
      "epoch:  0 train step:  232 loss:  1.8000868558883667 mini-batch correct:  39.0 train lr: 0.01\n",
      "epoch:  0 train step:  233 loss:  2.0381956100463867 mini-batch correct:  29.0 train lr: 0.01\n",
      "epoch:  0 train step:  234 loss:  1.7217440605163574 mini-batch correct:  44.0 train lr: 0.01\n",
      "epoch:  0 train step:  235 loss:  1.8937052488327026 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  236 loss:  1.8957940340042114 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  237 loss:  1.9223145246505737 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  238 loss:  1.8509495258331299 mini-batch correct:  36.0 train lr: 0.01\n",
      "epoch:  0 train step:  239 loss:  1.893425464630127 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  240 loss:  1.8979847431182861 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  241 loss:  1.8237898349761963 mini-batch correct:  50.0 train lr: 0.01\n",
      "epoch:  0 train step:  242 loss:  1.819656491279602 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  243 loss:  2.0061655044555664 mini-batch correct:  30.0 train lr: 0.01\n",
      "epoch:  0 train step:  244 loss:  1.873304843902588 mini-batch correct:  41.0 train lr: 0.01\n",
      "epoch:  0 train step:  245 loss:  1.909588098526001 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  246 loss:  1.8098421096801758 mini-batch correct:  40.0 train lr: 0.01\n",
      "epoch:  0 train step:  247 loss:  1.8914834260940552 mini-batch correct:  46.0 train lr: 0.01\n",
      "epoch:  0 train step:  248 loss:  1.9028098583221436 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  249 loss:  1.995816946029663 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  250 loss:  1.8549267053604126 mini-batch correct:  46.0 train lr: 0.01\n",
      "epoch:  0 train step:  251 loss:  2.00935435295105 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  252 loss:  1.8999130725860596 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  253 loss:  1.8455102443695068 mini-batch correct:  43.0 train lr: 0.01\n",
      "epoch:  0 train step:  254 loss:  1.9471417665481567 mini-batch correct:  30.0 train lr: 0.01\n",
      "epoch:  0 train step:  255 loss:  1.7878410816192627 mini-batch correct:  43.0 train lr: 0.01\n",
      "epoch:  0 train step:  256 loss:  2.0168039798736572 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  257 loss:  1.9513347148895264 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  258 loss:  1.8336763381958008 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  259 loss:  1.9521244764328003 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  260 loss:  1.8447262048721313 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  261 loss:  1.9272921085357666 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  262 loss:  1.8609061241149902 mini-batch correct:  34.0 train lr: 0.01\n",
      "epoch:  0 train step:  263 loss:  1.9405715465545654 mini-batch correct:  34.0 train lr: 0.01\n",
      "epoch:  0 train step:  264 loss:  1.997552752494812 mini-batch correct:  28.0 train lr: 0.01\n",
      "epoch:  0 train step:  265 loss:  1.8793790340423584 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  266 loss:  1.8898096084594727 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  267 loss:  2.017833948135376 mini-batch correct:  30.0 train lr: 0.01\n",
      "epoch:  0 train step:  268 loss:  1.8889501094818115 mini-batch correct:  28.0 train lr: 0.01\n",
      "epoch:  0 train step:  269 loss:  1.9893145561218262 mini-batch correct:  29.0 train lr: 0.01\n",
      "epoch:  0 train step:  270 loss:  1.9987679719924927 mini-batch correct:  26.0 train lr: 0.01\n",
      "epoch:  0 train step:  271 loss:  1.8943240642547607 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  272 loss:  1.9560315608978271 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  273 loss:  2.0036864280700684 mini-batch correct:  32.0 train lr: 0.01\n",
      "epoch:  0 train step:  274 loss:  1.9338178634643555 mini-batch correct:  40.0 train lr: 0.01\n",
      "epoch:  0 train step:  275 loss:  2.0068159103393555 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  276 loss:  1.91807222366333 mini-batch correct:  44.0 train lr: 0.01\n",
      "epoch:  0 train step:  277 loss:  1.9021940231323242 mini-batch correct:  40.0 train lr: 0.01\n",
      "epoch:  0 train step:  278 loss:  1.9755301475524902 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  279 loss:  1.967625379562378 mini-batch correct:  26.0 train lr: 0.01\n",
      "epoch:  0 train step:  280 loss:  1.7932844161987305 mini-batch correct:  44.0 train lr: 0.01\n",
      "epoch:  0 train step:  281 loss:  1.869769811630249 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  282 loss:  1.9070457220077515 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  283 loss:  1.8231250047683716 mini-batch correct:  45.0 train lr: 0.01\n",
      "epoch:  0 train step:  284 loss:  1.9286315441131592 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  285 loss:  1.9572398662567139 mini-batch correct:  36.0 train lr: 0.01\n",
      "epoch:  0 train step:  286 loss:  1.8033357858657837 mini-batch correct:  41.0 train lr: 0.01\n",
      "epoch:  0 train step:  287 loss:  1.912361979484558 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  288 loss:  1.8600720167160034 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  289 loss:  1.8744486570358276 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  290 loss:  1.957275629043579 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  291 loss:  1.8743393421173096 mini-batch correct:  39.0 train lr: 0.01\n",
      "epoch:  0 train step:  292 loss:  1.7910939455032349 mini-batch correct:  43.0 train lr: 0.01\n",
      "epoch:  0 train step:  293 loss:  1.839051604270935 mini-batch correct:  36.0 train lr: 0.01\n",
      "epoch:  0 train step:  294 loss:  1.8669565916061401 mini-batch correct:  45.0 train lr: 0.01\n",
      "epoch:  0 train step:  295 loss:  1.8978748321533203 mini-batch correct:  41.0 train lr: 0.01\n",
      "epoch:  0 train step:  296 loss:  1.982505202293396 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  297 loss:  1.804519772529602 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  298 loss:  1.837403655052185 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  299 loss:  1.7214725017547607 mini-batch correct:  48.0 train lr: 0.01\n",
      "epoch:  0 train step:  300 loss:  1.9198474884033203 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  301 loss:  1.9001941680908203 mini-batch correct:  41.0 train lr: 0.01\n",
      "epoch:  0 train step:  302 loss:  1.8126224279403687 mini-batch correct:  44.0 train lr: 0.01\n",
      "epoch:  0 train step:  303 loss:  1.7904983758926392 mini-batch correct:  36.0 train lr: 0.01\n",
      "epoch:  0 train step:  304 loss:  1.7562639713287354 mini-batch correct:  50.0 train lr: 0.01\n",
      "epoch:  0 train step:  305 loss:  1.9663867950439453 mini-batch correct:  44.0 train lr: 0.01\n",
      "epoch:  0 train step:  306 loss:  1.7239313125610352 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  307 loss:  1.8389525413513184 mini-batch correct:  34.0 train lr: 0.01\n",
      "epoch:  0 train step:  308 loss:  1.9463059902191162 mini-batch correct:  32.0 train lr: 0.01\n",
      "epoch:  0 train step:  309 loss:  1.7505484819412231 mini-batch correct:  44.0 train lr: 0.01\n",
      "epoch:  0 train step:  310 loss:  1.8426594734191895 mini-batch correct:  44.0 train lr: 0.01\n",
      "epoch:  0 train step:  311 loss:  1.8836313486099243 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  312 loss:  1.861540675163269 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  313 loss:  1.9369947910308838 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  314 loss:  1.7500863075256348 mini-batch correct:  45.0 train lr: 0.01\n",
      "epoch:  0 train step:  315 loss:  1.970182180404663 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  316 loss:  1.7704414129257202 mini-batch correct:  43.0 train lr: 0.01\n",
      "epoch:  0 train step:  317 loss:  1.871577501296997 mini-batch correct:  41.0 train lr: 0.01\n",
      "epoch:  0 train step:  318 loss:  1.7720149755477905 mini-batch correct:  43.0 train lr: 0.01\n",
      "epoch:  0 train step:  319 loss:  1.9517669677734375 mini-batch correct:  29.0 train lr: 0.01\n",
      "epoch:  0 train step:  320 loss:  1.8161362409591675 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  321 loss:  1.8445578813552856 mini-batch correct:  40.0 train lr: 0.01\n",
      "epoch:  0 train step:  322 loss:  1.893334150314331 mini-batch correct:  40.0 train lr: 0.01\n",
      "epoch:  0 train step:  323 loss:  1.8424912691116333 mini-batch correct:  41.0 train lr: 0.01\n",
      "epoch:  0 train step:  324 loss:  1.8038787841796875 mini-batch correct:  43.0 train lr: 0.01\n",
      "epoch:  0 train step:  325 loss:  1.9063102006912231 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  326 loss:  1.7927948236465454 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  327 loss:  1.8661954402923584 mini-batch correct:  49.0 train lr: 0.01\n",
      "epoch:  0 train step:  328 loss:  1.736432671546936 mini-batch correct:  49.0 train lr: 0.01\n",
      "epoch:  0 train step:  329 loss:  1.8470486402511597 mini-batch correct:  31.0 train lr: 0.01\n",
      "epoch:  0 train step:  330 loss:  1.7047827243804932 mini-batch correct:  55.0 train lr: 0.01\n",
      "epoch:  0 train step:  331 loss:  1.8000404834747314 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  332 loss:  1.918180227279663 mini-batch correct:  40.0 train lr: 0.01\n",
      "epoch:  0 train step:  333 loss:  1.863501787185669 mini-batch correct:  41.0 train lr: 0.01\n",
      "epoch:  0 train step:  334 loss:  1.876355767250061 mini-batch correct:  39.0 train lr: 0.01\n",
      "epoch:  0 train step:  335 loss:  2.0086426734924316 mini-batch correct:  40.0 train lr: 0.01\n",
      "epoch:  0 train step:  336 loss:  1.8865931034088135 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  337 loss:  1.9116650819778442 mini-batch correct:  41.0 train lr: 0.01\n",
      "epoch:  0 train step:  338 loss:  1.9987972974777222 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  339 loss:  1.678043007850647 mini-batch correct:  46.0 train lr: 0.01\n",
      "epoch:  0 train step:  340 loss:  1.9074015617370605 mini-batch correct:  34.0 train lr: 0.01\n",
      "epoch:  0 train step:  341 loss:  1.9007725715637207 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  342 loss:  1.771336317062378 mini-batch correct:  49.0 train lr: 0.01\n",
      "epoch:  0 train step:  343 loss:  1.9040474891662598 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  344 loss:  1.9250203371047974 mini-batch correct:  28.0 train lr: 0.01\n",
      "epoch:  0 train step:  345 loss:  1.8268697261810303 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  346 loss:  1.811446189880371 mini-batch correct:  39.0 train lr: 0.01\n",
      "epoch:  0 train step:  347 loss:  1.7337807416915894 mini-batch correct:  50.0 train lr: 0.01\n",
      "epoch:  0 train step:  348 loss:  1.6673221588134766 mini-batch correct:  54.0 train lr: 0.01\n",
      "epoch:  0 train step:  349 loss:  1.8109508752822876 mini-batch correct:  40.0 train lr: 0.01\n",
      "epoch:  0 train step:  350 loss:  1.7921141386032104 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  351 loss:  1.7571933269500732 mini-batch correct:  46.0 train lr: 0.01\n",
      "epoch:  0 train step:  352 loss:  1.892075538635254 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  353 loss:  1.9848724603652954 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  354 loss:  1.7853575944900513 mini-batch correct:  45.0 train lr: 0.01\n",
      "epoch:  0 train step:  355 loss:  1.9265180826187134 mini-batch correct:  48.0 train lr: 0.01\n",
      "epoch:  0 train step:  356 loss:  1.909104585647583 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  357 loss:  1.8566981554031372 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  358 loss:  1.891265630722046 mini-batch correct:  36.0 train lr: 0.01\n",
      "epoch:  0 train step:  359 loss:  1.8383949995040894 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  360 loss:  1.8362103700637817 mini-batch correct:  35.0 train lr: 0.01\n",
      "epoch:  0 train step:  361 loss:  1.7752528190612793 mini-batch correct:  39.0 train lr: 0.01\n",
      "epoch:  0 train step:  362 loss:  1.8078770637512207 mini-batch correct:  41.0 train lr: 0.01\n",
      "epoch:  0 train step:  363 loss:  1.840384840965271 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  364 loss:  1.9000661373138428 mini-batch correct:  37.0 train lr: 0.01\n",
      "epoch:  0 train step:  365 loss:  1.7923665046691895 mini-batch correct:  49.0 train lr: 0.01\n",
      "epoch:  0 train step:  366 loss:  1.8792616128921509 mini-batch correct:  36.0 train lr: 0.01\n",
      "epoch:  0 train step:  367 loss:  1.695920705795288 mini-batch correct:  53.0 train lr: 0.01\n",
      "epoch:  0 train step:  368 loss:  1.763102412223816 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  369 loss:  1.8207917213439941 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  370 loss:  1.792785406112671 mini-batch correct:  48.0 train lr: 0.01\n",
      "epoch:  0 train step:  371 loss:  1.8186978101730347 mini-batch correct:  39.0 train lr: 0.01\n",
      "epoch:  0 train step:  372 loss:  2.005638599395752 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  373 loss:  1.8417558670043945 mini-batch correct:  40.0 train lr: 0.01\n",
      "epoch:  0 train step:  374 loss:  1.832861304283142 mini-batch correct:  43.0 train lr: 0.01\n",
      "epoch:  0 train step:  375 loss:  1.7846736907958984 mini-batch correct:  42.0 train lr: 0.01\n",
      "epoch:  0 train step:  376 loss:  1.7800720930099487 mini-batch correct:  43.0 train lr: 0.01\n",
      "epoch:  0 train step:  377 loss:  1.8338382244110107 mini-batch correct:  43.0 train lr: 0.01\n",
      "epoch:  0 train step:  378 loss:  1.8076403141021729 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  379 loss:  1.7858906984329224 mini-batch correct:  44.0 train lr: 0.01\n",
      "epoch:  0 train step:  380 loss:  1.8346292972564697 mini-batch correct:  33.0 train lr: 0.01\n",
      "epoch:  0 train step:  381 loss:  1.6949570178985596 mini-batch correct:  47.0 train lr: 0.01\n",
      "epoch:  0 train step:  382 loss:  1.7407540082931519 mini-batch correct:  40.0 train lr: 0.01\n",
      "epoch:  0 train step:  383 loss:  1.8026190996170044 mini-batch correct:  40.0 train lr: 0.01\n",
      "epoch:  0 train step:  384 loss:  1.826100468635559 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  385 loss:  1.783961534500122 mini-batch correct:  45.0 train lr: 0.01\n",
      "epoch:  0 train step:  386 loss:  1.7605446577072144 mini-batch correct:  39.0 train lr: 0.01\n",
      "epoch:  0 train step:  387 loss:  1.854528546333313 mini-batch correct:  38.0 train lr: 0.01\n",
      "epoch:  0 train step:  388 loss:  1.8347879648208618 mini-batch correct:  47.0 train lr: 0.01\n",
      "epoch:  0 train step:  389 loss:  1.7735519409179688 mini-batch correct:  48.0 train lr: 0.01\n",
      "epoch:  0 train step:  390 loss:  1.6925277709960938 mini-batch correct:  26.0 train lr: 0.01\n",
      "epoch: 0 loss:  1.7290193461164642 test correct:  44.72151898734177\n",
      "epoch is:  1\n",
      "epoch:  1 train step:  0 loss:  1.863067865371704 mini-batch correct:  38.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  1 loss:  1.860893964767456 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  2 loss:  1.8150291442871094 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  3 loss:  1.820563554763794 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  4 loss:  1.7886078357696533 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  5 loss:  1.7055253982543945 mini-batch correct:  54.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  6 loss:  1.88228178024292 mini-batch correct:  39.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  7 loss:  1.812735915184021 mini-batch correct:  38.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  8 loss:  1.8799312114715576 mini-batch correct:  38.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  9 loss:  1.783673882484436 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  10 loss:  1.6701948642730713 mini-batch correct:  54.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  11 loss:  1.8203332424163818 mini-batch correct:  38.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  12 loss:  1.7678592205047607 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  13 loss:  1.762789249420166 mini-batch correct:  52.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  14 loss:  1.762168049812317 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  15 loss:  1.8730601072311401 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  16 loss:  1.7746156454086304 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  17 loss:  1.8202838897705078 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  18 loss:  1.8814254999160767 mini-batch correct:  37.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  19 loss:  1.8218870162963867 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  20 loss:  1.8311654329299927 mini-batch correct:  39.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  21 loss:  1.847137451171875 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  22 loss:  1.8352524042129517 mini-batch correct:  51.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  23 loss:  1.686612844467163 mini-batch correct:  52.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  24 loss:  1.898171305656433 mini-batch correct:  39.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  25 loss:  1.799410104751587 mini-batch correct:  34.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  26 loss:  1.7436497211456299 mini-batch correct:  39.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  27 loss:  1.7267416715621948 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  28 loss:  1.6034573316574097 mini-batch correct:  53.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  29 loss:  1.7571240663528442 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  30 loss:  1.9169719219207764 mini-batch correct:  32.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  31 loss:  1.8650813102722168 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  32 loss:  1.8104933500289917 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  33 loss:  1.7369059324264526 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  34 loss:  1.8036839962005615 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  35 loss:  1.8970565795898438 mini-batch correct:  35.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  36 loss:  1.779738426208496 mini-batch correct:  37.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  37 loss:  1.8983005285263062 mini-batch correct:  35.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  38 loss:  1.8928916454315186 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  39 loss:  1.8821452856063843 mini-batch correct:  39.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  40 loss:  1.874088168144226 mini-batch correct:  36.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  41 loss:  1.7805070877075195 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  42 loss:  1.7692577838897705 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  43 loss:  1.7547050714492798 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  44 loss:  1.7484136819839478 mini-batch correct:  39.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  45 loss:  1.8586883544921875 mini-batch correct:  38.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  46 loss:  1.8391296863555908 mini-batch correct:  37.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  47 loss:  1.7873455286026 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  48 loss:  1.8410141468048096 mini-batch correct:  38.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  49 loss:  1.8224236965179443 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  50 loss:  1.7811851501464844 mini-batch correct:  37.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  51 loss:  1.7721518278121948 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  52 loss:  1.7673643827438354 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  53 loss:  1.7693151235580444 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  54 loss:  1.7428474426269531 mini-batch correct:  38.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  55 loss:  1.8803293704986572 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  56 loss:  1.8887360095977783 mini-batch correct:  39.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  57 loss:  1.8998783826828003 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  58 loss:  1.8466213941574097 mini-batch correct:  35.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  59 loss:  1.7430883646011353 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  60 loss:  1.746543526649475 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  61 loss:  1.7777870893478394 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  62 loss:  1.963246464729309 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  63 loss:  1.7129487991333008 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  64 loss:  1.7938342094421387 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  65 loss:  1.8101507425308228 mini-batch correct:  43.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  66 loss:  1.8489025831222534 mini-batch correct:  41.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  67 loss:  1.8420823812484741 mini-batch correct:  35.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  68 loss:  1.7898313999176025 mini-batch correct:  39.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  69 loss:  1.7332240343093872 mini-batch correct:  39.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  70 loss:  1.6780532598495483 mini-batch correct:  53.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  71 loss:  1.8436211347579956 mini-batch correct:  37.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  72 loss:  1.7248730659484863 mini-batch correct:  43.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  73 loss:  1.9641978740692139 mini-batch correct:  32.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  74 loss:  1.7708520889282227 mini-batch correct:  53.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  75 loss:  1.7201364040374756 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  76 loss:  1.8559157848358154 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  77 loss:  1.8552535772323608 mini-batch correct:  36.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  78 loss:  1.701972246170044 mini-batch correct:  52.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  79 loss:  1.7750334739685059 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  80 loss:  1.8113445043563843 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  81 loss:  1.8342583179473877 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  82 loss:  1.7199848890304565 mini-batch correct:  51.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  83 loss:  1.7814518213272095 mini-batch correct:  43.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  84 loss:  1.8270214796066284 mini-batch correct:  39.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  85 loss:  1.9644895792007446 mini-batch correct:  37.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  86 loss:  1.8121598958969116 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  87 loss:  1.6610275506973267 mini-batch correct:  53.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  88 loss:  1.751237154006958 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  89 loss:  1.9254976511001587 mini-batch correct:  38.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  90 loss:  1.785212516784668 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  91 loss:  1.75945246219635 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  92 loss:  1.7780312299728394 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  93 loss:  1.6438453197479248 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  94 loss:  1.7380605936050415 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  95 loss:  1.8691478967666626 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  96 loss:  1.763379454612732 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  97 loss:  1.689657211303711 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  98 loss:  1.866396188735962 mini-batch correct:  38.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  99 loss:  1.855141520500183 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  100 loss:  1.6844477653503418 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  101 loss:  1.739517092704773 mini-batch correct:  49.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  102 loss:  1.776599645614624 mini-batch correct:  43.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  103 loss:  1.7650134563446045 mini-batch correct:  39.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  104 loss:  1.98112154006958 mini-batch correct:  41.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  105 loss:  1.9013516902923584 mini-batch correct:  41.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  106 loss:  1.7238032817840576 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  107 loss:  1.6750015020370483 mini-batch correct:  52.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  108 loss:  1.960170865058899 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  109 loss:  1.7962067127227783 mini-batch correct:  37.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  110 loss:  1.7136626243591309 mini-batch correct:  41.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  111 loss:  1.852286458015442 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  112 loss:  1.7316100597381592 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  113 loss:  1.7618821859359741 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  114 loss:  1.8334019184112549 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  115 loss:  1.8983910083770752 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  116 loss:  1.8760793209075928 mini-batch correct:  30.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  117 loss:  1.7468552589416504 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  118 loss:  1.7897790670394897 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  119 loss:  1.6026731729507446 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  120 loss:  1.644849419593811 mini-batch correct:  59.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  121 loss:  1.7787431478500366 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  122 loss:  1.8449091911315918 mini-batch correct:  51.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  123 loss:  1.6761181354522705 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  124 loss:  1.7088462114334106 mini-batch correct:  41.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  125 loss:  1.7294903993606567 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  126 loss:  1.8859598636627197 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  127 loss:  1.8136762380599976 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  128 loss:  1.6626344919204712 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  129 loss:  1.9041318893432617 mini-batch correct:  41.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  130 loss:  1.827291488647461 mini-batch correct:  33.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  131 loss:  1.9136136770248413 mini-batch correct:  37.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  132 loss:  1.859527349472046 mini-batch correct:  41.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  133 loss:  1.642920732498169 mini-batch correct:  57.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  134 loss:  1.8437379598617554 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  135 loss:  1.721755027770996 mini-batch correct:  53.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  136 loss:  1.785727620124817 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  137 loss:  1.7805774211883545 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  138 loss:  1.871647834777832 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  139 loss:  1.71090829372406 mini-batch correct:  43.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  140 loss:  1.779070496559143 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  141 loss:  1.8596017360687256 mini-batch correct:  36.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  142 loss:  1.6796904802322388 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  143 loss:  1.7551155090332031 mini-batch correct:  49.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  144 loss:  1.7076001167297363 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  145 loss:  1.8318426609039307 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  146 loss:  1.7467149496078491 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  147 loss:  1.8232324123382568 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  148 loss:  1.7078458070755005 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  149 loss:  1.7123454809188843 mini-batch correct:  38.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  150 loss:  1.6951431035995483 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  151 loss:  1.7729880809783936 mini-batch correct:  51.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  152 loss:  1.6092159748077393 mini-batch correct:  52.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  153 loss:  1.6566725969314575 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  154 loss:  1.7632157802581787 mini-batch correct:  49.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  155 loss:  1.7472972869873047 mini-batch correct:  43.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  156 loss:  1.6398799419403076 mini-batch correct:  49.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  157 loss:  1.9446593523025513 mini-batch correct:  36.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  158 loss:  1.7886995077133179 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  159 loss:  1.5724825859069824 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  160 loss:  1.8303502798080444 mini-batch correct:  37.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  161 loss:  1.6987255811691284 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  162 loss:  1.7225321531295776 mini-batch correct:  41.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  163 loss:  1.8272464275360107 mini-batch correct:  41.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  164 loss:  1.569128394126892 mini-batch correct:  56.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  165 loss:  1.663196086883545 mini-batch correct:  56.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  166 loss:  1.801677942276001 mini-batch correct:  37.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  167 loss:  1.7152552604675293 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  168 loss:  1.6594722270965576 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  169 loss:  1.7131874561309814 mini-batch correct:  53.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  170 loss:  1.7497587203979492 mini-batch correct:  55.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  171 loss:  1.7423439025878906 mini-batch correct:  43.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  172 loss:  1.759713053703308 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  173 loss:  1.7512130737304688 mini-batch correct:  43.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  174 loss:  1.8031457662582397 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  175 loss:  1.7777774333953857 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  176 loss:  1.767080545425415 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  177 loss:  1.7761225700378418 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  178 loss:  1.6364127397537231 mini-batch correct:  53.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  179 loss:  1.7556639909744263 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  180 loss:  1.8807989358901978 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  181 loss:  1.686151385307312 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  182 loss:  1.781766414642334 mini-batch correct:  41.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  183 loss:  1.7520911693572998 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  184 loss:  1.5912562608718872 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  185 loss:  1.701842188835144 mini-batch correct:  54.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  186 loss:  1.7141636610031128 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  187 loss:  1.7411710023880005 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  188 loss:  1.6802901029586792 mini-batch correct:  49.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  189 loss:  1.9309190511703491 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  190 loss:  1.7886192798614502 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  191 loss:  2.070495367050171 mini-batch correct:  36.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  192 loss:  1.7571067810058594 mini-batch correct:  51.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  193 loss:  1.7598817348480225 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  194 loss:  1.6654808521270752 mini-batch correct:  49.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  195 loss:  1.7724939584732056 mini-batch correct:  41.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  196 loss:  1.7811095714569092 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  197 loss:  1.8696200847625732 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  198 loss:  1.717200756072998 mini-batch correct:  54.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  199 loss:  1.7344614267349243 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  200 loss:  1.7952519655227661 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  201 loss:  1.8006055355072021 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  202 loss:  1.8975590467453003 mini-batch correct:  41.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  203 loss:  1.8086206912994385 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  204 loss:  1.696958065032959 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  205 loss:  1.5859088897705078 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  206 loss:  1.8287792205810547 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  207 loss:  1.763143539428711 mini-batch correct:  54.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  208 loss:  1.7890167236328125 mini-batch correct:  49.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  209 loss:  1.8247991800308228 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  210 loss:  1.7067471742630005 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  211 loss:  1.6593186855316162 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  212 loss:  1.7451558113098145 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  213 loss:  1.6756104230880737 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  214 loss:  1.634898066520691 mini-batch correct:  54.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  215 loss:  1.711874008178711 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  216 loss:  1.6866878271102905 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  217 loss:  1.7796710729599 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  218 loss:  1.7461003065109253 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  219 loss:  1.759311556816101 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  220 loss:  1.6786619424819946 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  221 loss:  1.7072700262069702 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  222 loss:  1.8552545309066772 mini-batch correct:  43.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  223 loss:  1.6148947477340698 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  224 loss:  1.7049195766448975 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  225 loss:  1.8264532089233398 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  226 loss:  1.7646013498306274 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  227 loss:  1.8741700649261475 mini-batch correct:  38.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  228 loss:  1.7189046144485474 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  229 loss:  1.6930688619613647 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  230 loss:  1.8044813871383667 mini-batch correct:  41.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  231 loss:  1.7800283432006836 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  232 loss:  1.7241311073303223 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  233 loss:  1.9053510427474976 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  234 loss:  1.7185139656066895 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  235 loss:  1.8311104774475098 mini-batch correct:  41.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  236 loss:  1.7010042667388916 mini-batch correct:  52.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  237 loss:  1.76291823387146 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  238 loss:  1.9085034132003784 mini-batch correct:  36.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  239 loss:  1.70371675491333 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  240 loss:  1.6678473949432373 mini-batch correct:  56.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  241 loss:  1.7890359163284302 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  242 loss:  1.7670037746429443 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  243 loss:  1.6540772914886475 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  244 loss:  1.680808186531067 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  245 loss:  1.7110931873321533 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  246 loss:  1.720750331878662 mini-batch correct:  52.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  247 loss:  1.7125860452651978 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  248 loss:  1.7721316814422607 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  249 loss:  1.7581409215927124 mini-batch correct:  43.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  250 loss:  1.693408727645874 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  251 loss:  1.6099690198898315 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  252 loss:  1.653883934020996 mini-batch correct:  54.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  253 loss:  1.7918825149536133 mini-batch correct:  41.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  254 loss:  1.7520804405212402 mini-batch correct:  54.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  255 loss:  1.7492926120758057 mini-batch correct:  49.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  256 loss:  1.6435308456420898 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  257 loss:  1.6967614889144897 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  258 loss:  1.7256709337234497 mini-batch correct:  49.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  259 loss:  1.7163519859313965 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  260 loss:  1.732421636581421 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  261 loss:  1.5600312948226929 mini-batch correct:  60.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  262 loss:  1.8261222839355469 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  263 loss:  1.6617075204849243 mini-batch correct:  43.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  264 loss:  1.876417875289917 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  265 loss:  1.6989386081695557 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  266 loss:  1.86530339717865 mini-batch correct:  43.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  267 loss:  1.6732690334320068 mini-batch correct:  51.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  268 loss:  1.8232117891311646 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  269 loss:  1.6341263055801392 mini-batch correct:  54.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  270 loss:  1.688302993774414 mini-batch correct:  40.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  271 loss:  1.8382477760314941 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  272 loss:  1.6978732347488403 mini-batch correct:  51.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  273 loss:  1.7933293581008911 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  274 loss:  1.7850091457366943 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  275 loss:  1.652449131011963 mini-batch correct:  57.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  276 loss:  1.6821353435516357 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  277 loss:  1.8403701782226562 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  278 loss:  1.649083137512207 mini-batch correct:  56.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  279 loss:  1.806288480758667 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  280 loss:  1.6985079050064087 mini-batch correct:  51.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  281 loss:  1.6714283227920532 mini-batch correct:  52.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  282 loss:  1.7750402688980103 mini-batch correct:  41.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  283 loss:  1.6292613744735718 mini-batch correct:  53.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  284 loss:  1.7035025358200073 mini-batch correct:  55.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  285 loss:  1.7065403461456299 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  286 loss:  1.7758668661117554 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  287 loss:  1.6095994710922241 mini-batch correct:  59.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  288 loss:  1.7060579061508179 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  289 loss:  1.7151752710342407 mini-batch correct:  43.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  290 loss:  1.8367252349853516 mini-batch correct:  39.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  291 loss:  1.8327529430389404 mini-batch correct:  43.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  292 loss:  1.5182992219924927 mini-batch correct:  60.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  293 loss:  1.6748706102371216 mini-batch correct:  57.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  294 loss:  1.8088675737380981 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  295 loss:  1.6439793109893799 mini-batch correct:  55.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  296 loss:  1.5810847282409668 mini-batch correct:  54.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  297 loss:  1.7072800397872925 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  298 loss:  1.6124987602233887 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  299 loss:  1.6705933809280396 mini-batch correct:  55.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  300 loss:  1.7187477350234985 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  301 loss:  1.7887643575668335 mini-batch correct:  41.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  302 loss:  1.6507132053375244 mini-batch correct:  52.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  303 loss:  1.7005462646484375 mini-batch correct:  49.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  304 loss:  1.6439685821533203 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  305 loss:  1.774139165878296 mini-batch correct:  36.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  306 loss:  1.7450941801071167 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  307 loss:  1.6714805364608765 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  308 loss:  1.6294149160385132 mini-batch correct:  51.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  309 loss:  1.6321762800216675 mini-batch correct:  52.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  310 loss:  1.6679704189300537 mini-batch correct:  53.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  311 loss:  1.7442020177841187 mini-batch correct:  43.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  312 loss:  1.7470015287399292 mini-batch correct:  53.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  313 loss:  1.5341228246688843 mini-batch correct:  53.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  314 loss:  1.6371636390686035 mini-batch correct:  59.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  315 loss:  1.733292579650879 mini-batch correct:  54.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  316 loss:  1.6862869262695312 mini-batch correct:  54.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  317 loss:  1.6279098987579346 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  318 loss:  1.5392017364501953 mini-batch correct:  60.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  319 loss:  1.6491563320159912 mini-batch correct:  57.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  320 loss:  1.9013444185256958 mini-batch correct:  43.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  321 loss:  1.7695637941360474 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  322 loss:  1.7668999433517456 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  323 loss:  1.6457093954086304 mini-batch correct:  55.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  324 loss:  1.694496512413025 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  325 loss:  1.6147652864456177 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  326 loss:  1.824218988418579 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  327 loss:  1.6600074768066406 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  328 loss:  1.6831215620040894 mini-batch correct:  54.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  329 loss:  1.6589887142181396 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  330 loss:  1.5478451251983643 mini-batch correct:  64.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  331 loss:  1.7593486309051514 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  332 loss:  1.7998430728912354 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  333 loss:  1.6672391891479492 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  334 loss:  1.6931216716766357 mini-batch correct:  54.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  335 loss:  1.6440577507019043 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  336 loss:  1.7558525800704956 mini-batch correct:  49.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  337 loss:  1.7976963520050049 mini-batch correct:  52.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  338 loss:  1.596765160560608 mini-batch correct:  57.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  339 loss:  1.687302827835083 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  340 loss:  1.683096170425415 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  341 loss:  1.6057535409927368 mini-batch correct:  54.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  342 loss:  1.8005881309509277 mini-batch correct:  46.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  343 loss:  1.684956669807434 mini-batch correct:  54.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  344 loss:  1.733938217163086 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  345 loss:  1.7000341415405273 mini-batch correct:  38.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  346 loss:  1.5763051509857178 mini-batch correct:  59.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  347 loss:  1.8764903545379639 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  348 loss:  1.7095478773117065 mini-batch correct:  52.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  349 loss:  1.781449794769287 mini-batch correct:  41.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  350 loss:  1.9137877225875854 mini-batch correct:  41.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  351 loss:  1.6755635738372803 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  352 loss:  1.6933784484863281 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  353 loss:  1.7584140300750732 mini-batch correct:  37.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  354 loss:  1.6271469593048096 mini-batch correct:  51.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  355 loss:  1.6595295667648315 mini-batch correct:  59.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  356 loss:  1.768519401550293 mini-batch correct:  44.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  357 loss:  1.742318868637085 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  358 loss:  1.8239201307296753 mini-batch correct:  35.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  359 loss:  1.8281186819076538 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  360 loss:  1.9443379640579224 mini-batch correct:  32.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  361 loss:  1.7309316396713257 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  362 loss:  1.620912790298462 mini-batch correct:  51.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  363 loss:  1.7089966535568237 mini-batch correct:  49.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  364 loss:  1.5801353454589844 mini-batch correct:  58.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  365 loss:  1.7271450757980347 mini-batch correct:  51.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  366 loss:  1.5579955577850342 mini-batch correct:  51.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  367 loss:  1.758764386177063 mini-batch correct:  43.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  368 loss:  1.7187557220458984 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  369 loss:  1.5386662483215332 mini-batch correct:  54.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  370 loss:  1.7419242858886719 mini-batch correct:  51.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  371 loss:  1.6979976892471313 mini-batch correct:  49.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  372 loss:  1.6851229667663574 mini-batch correct:  51.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  373 loss:  1.6380728483200073 mini-batch correct:  59.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  374 loss:  1.7733957767486572 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  375 loss:  1.6968564987182617 mini-batch correct:  47.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  376 loss:  1.7483890056610107 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  377 loss:  1.603912353515625 mini-batch correct:  56.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  378 loss:  1.6350722312927246 mini-batch correct:  57.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  379 loss:  1.6119015216827393 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  380 loss:  1.6353349685668945 mini-batch correct:  51.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  381 loss:  1.7441691160202026 mini-batch correct:  43.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  382 loss:  1.6558268070220947 mini-batch correct:  53.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  383 loss:  1.7593059539794922 mini-batch correct:  39.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  384 loss:  1.6662724018096924 mini-batch correct:  45.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  385 loss:  1.5380011796951294 mini-batch correct:  53.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  386 loss:  1.794250249862671 mini-batch correct:  42.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  387 loss:  1.6740142107009888 mini-batch correct:  48.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  388 loss:  1.5917414426803589 mini-batch correct:  54.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  389 loss:  1.7063772678375244 mini-batch correct:  50.0 train lr: 0.009000000000000001\n",
      "epoch:  1 train step:  390 loss:  1.7466071844100952 mini-batch correct:  25.0 train lr: 0.009000000000000001\n",
      "epoch: 1 loss:  1.5558386045166208 test correct:  54.75949367088608\n",
      "epoch is:  2\n",
      "epoch:  2 train step:  0 loss:  1.6188894510269165 mini-batch correct:  49.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  1 loss:  1.6933715343475342 mini-batch correct:  49.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  2 loss:  1.5942580699920654 mini-batch correct:  44.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  3 loss:  1.6538102626800537 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  4 loss:  1.8063631057739258 mini-batch correct:  40.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  5 loss:  1.728562355041504 mini-batch correct:  41.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  6 loss:  1.737622857093811 mini-batch correct:  49.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  7 loss:  1.7404977083206177 mini-batch correct:  47.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  8 loss:  1.5386278629302979 mini-batch correct:  60.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  9 loss:  1.7256669998168945 mini-batch correct:  41.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  10 loss:  1.6440171003341675 mini-batch correct:  55.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  11 loss:  1.6287225484848022 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  12 loss:  1.6376867294311523 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  13 loss:  1.6554508209228516 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  14 loss:  1.5281299352645874 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  15 loss:  1.5750056505203247 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  16 loss:  1.7231281995773315 mini-batch correct:  43.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  17 loss:  1.5995993614196777 mini-batch correct:  58.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  18 loss:  1.7433602809906006 mini-batch correct:  46.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  19 loss:  1.6912356615066528 mini-batch correct:  47.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  20 loss:  1.6282711029052734 mini-batch correct:  55.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  21 loss:  1.7863389253616333 mini-batch correct:  43.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  22 loss:  1.6140145063400269 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  23 loss:  1.688102126121521 mini-batch correct:  42.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  24 loss:  1.865256667137146 mini-batch correct:  43.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  25 loss:  1.5759153366088867 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  26 loss:  1.6343584060668945 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  27 loss:  1.5442291498184204 mini-batch correct:  50.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  28 loss:  1.6122987270355225 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  29 loss:  1.590409517288208 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  30 loss:  1.5682452917099 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  31 loss:  1.688733696937561 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  32 loss:  1.5699926614761353 mini-batch correct:  50.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  33 loss:  1.6414752006530762 mini-batch correct:  41.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  34 loss:  1.6269567012786865 mini-batch correct:  50.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  35 loss:  1.9257338047027588 mini-batch correct:  41.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  36 loss:  1.7033542394638062 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  37 loss:  1.5988731384277344 mini-batch correct:  48.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  38 loss:  1.532598614692688 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  39 loss:  1.8326704502105713 mini-batch correct:  42.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  40 loss:  1.6339125633239746 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  41 loss:  1.6291815042495728 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  42 loss:  1.8730233907699585 mini-batch correct:  38.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  43 loss:  1.6919347047805786 mini-batch correct:  49.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  44 loss:  1.6496068239212036 mini-batch correct:  55.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  45 loss:  1.7019476890563965 mini-batch correct:  47.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  46 loss:  1.472914695739746 mini-batch correct:  64.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  47 loss:  1.6898256540298462 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  48 loss:  1.6485475301742554 mini-batch correct:  49.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  49 loss:  1.674049973487854 mini-batch correct:  48.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  50 loss:  1.658387541770935 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  51 loss:  1.6204683780670166 mini-batch correct:  49.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  52 loss:  1.5891828536987305 mini-batch correct:  50.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  53 loss:  1.5460275411605835 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  54 loss:  1.5801961421966553 mini-batch correct:  55.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  55 loss:  1.6681417226791382 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  56 loss:  1.742914080619812 mini-batch correct:  45.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  57 loss:  1.6400911808013916 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  58 loss:  1.7362316846847534 mini-batch correct:  46.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  59 loss:  1.697713017463684 mini-batch correct:  44.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  60 loss:  1.7290754318237305 mini-batch correct:  44.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  61 loss:  1.6085764169692993 mini-batch correct:  46.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  62 loss:  1.7802414894104004 mini-batch correct:  47.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  63 loss:  1.5507599115371704 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  64 loss:  1.7900595664978027 mini-batch correct:  43.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  65 loss:  1.6518592834472656 mini-batch correct:  48.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  66 loss:  1.5682443380355835 mini-batch correct:  59.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  67 loss:  1.7161105871200562 mini-batch correct:  41.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  68 loss:  1.619434118270874 mini-batch correct:  60.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  69 loss:  1.730536937713623 mini-batch correct:  50.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  70 loss:  1.697417974472046 mini-batch correct:  47.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  71 loss:  1.6002063751220703 mini-batch correct:  42.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  72 loss:  1.7022204399108887 mini-batch correct:  49.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  73 loss:  1.5369478464126587 mini-batch correct:  59.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  74 loss:  1.7557796239852905 mini-batch correct:  43.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  75 loss:  1.6049952507019043 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  76 loss:  1.7635278701782227 mini-batch correct:  40.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  77 loss:  1.6458137035369873 mini-batch correct:  48.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  78 loss:  1.6697815656661987 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  79 loss:  1.5431694984436035 mini-batch correct:  50.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  80 loss:  1.7305480241775513 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  81 loss:  1.7146008014678955 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  82 loss:  1.7892998456954956 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  83 loss:  1.7176624536514282 mini-batch correct:  48.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  84 loss:  1.618536114692688 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  85 loss:  1.650293231010437 mini-batch correct:  50.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  86 loss:  1.6389447450637817 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  87 loss:  1.651673674583435 mini-batch correct:  49.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  88 loss:  1.6376872062683105 mini-batch correct:  50.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  89 loss:  1.5468003749847412 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  90 loss:  1.6351041793823242 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  91 loss:  1.756447196006775 mini-batch correct:  49.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  92 loss:  1.6544408798217773 mini-batch correct:  44.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  93 loss:  1.6534782648086548 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  94 loss:  1.6962733268737793 mini-batch correct:  47.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  95 loss:  1.7108725309371948 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  96 loss:  1.7618271112442017 mini-batch correct:  43.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  97 loss:  1.580565094947815 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  98 loss:  1.6164137125015259 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  99 loss:  1.566798210144043 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  100 loss:  1.6744197607040405 mini-batch correct:  42.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  101 loss:  1.7160496711730957 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  102 loss:  1.7280912399291992 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  103 loss:  1.5863193273544312 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  104 loss:  1.7368860244750977 mini-batch correct:  40.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  105 loss:  1.6580774784088135 mini-batch correct:  45.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  106 loss:  1.541846513748169 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  107 loss:  1.5157856941223145 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  108 loss:  1.6005791425704956 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  109 loss:  1.5395716428756714 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  110 loss:  1.6273324489593506 mini-batch correct:  58.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  111 loss:  1.7355819940567017 mini-batch correct:  47.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  112 loss:  1.4950802326202393 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  113 loss:  1.7404191493988037 mini-batch correct:  46.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  114 loss:  1.5909075736999512 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  115 loss:  1.6427596807479858 mini-batch correct:  46.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  116 loss:  1.7388273477554321 mini-batch correct:  50.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  117 loss:  1.6188455820083618 mini-batch correct:  62.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  118 loss:  1.6634052991867065 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  119 loss:  1.6376935243606567 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  120 loss:  1.6205869913101196 mini-batch correct:  44.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  121 loss:  1.5760773420333862 mini-batch correct:  64.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  122 loss:  1.7152137756347656 mini-batch correct:  49.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  123 loss:  1.7506989240646362 mini-batch correct:  48.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  124 loss:  1.6246838569641113 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  125 loss:  1.5834535360336304 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  126 loss:  1.796789288520813 mini-batch correct:  46.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  127 loss:  1.5833371877670288 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  128 loss:  1.5389816761016846 mini-batch correct:  60.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  129 loss:  1.7158502340316772 mini-batch correct:  50.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  130 loss:  1.7203377485275269 mini-batch correct:  47.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  131 loss:  1.5797207355499268 mini-batch correct:  50.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  132 loss:  1.5403683185577393 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  133 loss:  1.6428500413894653 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  134 loss:  1.7645107507705688 mini-batch correct:  47.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  135 loss:  1.6882740259170532 mini-batch correct:  45.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  136 loss:  1.6235712766647339 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  137 loss:  1.5827723741531372 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  138 loss:  1.5426007509231567 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  139 loss:  1.6621721982955933 mini-batch correct:  50.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  140 loss:  1.5495935678482056 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  141 loss:  1.556990385055542 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  142 loss:  1.6413788795471191 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  143 loss:  1.618133544921875 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  144 loss:  1.4750598669052124 mini-batch correct:  58.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  145 loss:  1.7337799072265625 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  146 loss:  1.570046305656433 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  147 loss:  1.5872159004211426 mini-batch correct:  55.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  148 loss:  1.5950795412063599 mini-batch correct:  60.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  149 loss:  1.5867310762405396 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  150 loss:  1.6246609687805176 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  151 loss:  1.6059726476669312 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  152 loss:  1.7452125549316406 mini-batch correct:  46.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  153 loss:  1.6229768991470337 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  154 loss:  1.7244179248809814 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  155 loss:  1.5679057836532593 mini-batch correct:  59.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  156 loss:  1.686300277709961 mini-batch correct:  48.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  157 loss:  1.5982872247695923 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  158 loss:  1.5609939098358154 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  159 loss:  1.5947294235229492 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  160 loss:  1.6029375791549683 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  161 loss:  1.4884732961654663 mini-batch correct:  65.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  162 loss:  1.785127878189087 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  163 loss:  1.5648194551467896 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  164 loss:  1.6006183624267578 mini-batch correct:  50.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  165 loss:  1.689212679862976 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  166 loss:  1.6888021230697632 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  167 loss:  1.5794614553451538 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  168 loss:  1.7660400867462158 mini-batch correct:  47.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  169 loss:  1.5941522121429443 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  170 loss:  1.724990725517273 mini-batch correct:  42.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  171 loss:  1.4555115699768066 mini-batch correct:  61.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  172 loss:  1.6391969919204712 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  173 loss:  1.652010202407837 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  174 loss:  1.7099199295043945 mini-batch correct:  46.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  175 loss:  1.5809509754180908 mini-batch correct:  59.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  176 loss:  1.6413720846176147 mini-batch correct:  49.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  177 loss:  1.685174822807312 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  178 loss:  1.5476906299591064 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  179 loss:  1.7756041288375854 mini-batch correct:  48.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  180 loss:  1.5876694917678833 mini-batch correct:  62.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  181 loss:  1.6277292966842651 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  182 loss:  1.5620884895324707 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  183 loss:  1.5632860660552979 mini-batch correct:  49.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  184 loss:  1.4451061487197876 mini-batch correct:  62.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  185 loss:  1.6663621664047241 mini-batch correct:  45.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  186 loss:  1.5924124717712402 mini-batch correct:  45.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  187 loss:  1.5830881595611572 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  188 loss:  1.640428900718689 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  189 loss:  1.489917516708374 mini-batch correct:  64.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  190 loss:  1.6583608388900757 mini-batch correct:  49.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  191 loss:  1.6872965097427368 mini-batch correct:  47.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  192 loss:  1.681188702583313 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  193 loss:  1.5465742349624634 mini-batch correct:  58.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  194 loss:  1.540197730064392 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  195 loss:  1.5211237668991089 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  196 loss:  1.5209736824035645 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  197 loss:  1.651424527168274 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  198 loss:  1.51392662525177 mini-batch correct:  64.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  199 loss:  1.6487386226654053 mini-batch correct:  49.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  200 loss:  1.5329042673110962 mini-batch correct:  58.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  201 loss:  1.6093409061431885 mini-batch correct:  58.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  202 loss:  1.5559265613555908 mini-batch correct:  59.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  203 loss:  1.627597451210022 mini-batch correct:  45.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  204 loss:  1.4995814561843872 mini-batch correct:  61.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  205 loss:  1.7424484491348267 mini-batch correct:  43.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  206 loss:  1.453674077987671 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  207 loss:  1.6110568046569824 mini-batch correct:  48.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  208 loss:  1.6445614099502563 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  209 loss:  1.7217986583709717 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  210 loss:  1.6432281732559204 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  211 loss:  1.6197803020477295 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  212 loss:  1.5185261964797974 mini-batch correct:  59.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  213 loss:  1.5529626607894897 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  214 loss:  1.563183069229126 mini-batch correct:  61.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  215 loss:  1.7150596380233765 mini-batch correct:  47.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  216 loss:  1.5576058626174927 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  217 loss:  1.6128060817718506 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  218 loss:  1.5750668048858643 mini-batch correct:  61.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  219 loss:  1.6468905210494995 mini-batch correct:  48.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  220 loss:  1.7046716213226318 mini-batch correct:  50.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  221 loss:  1.6564176082611084 mini-batch correct:  47.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  222 loss:  1.807695746421814 mini-batch correct:  43.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  223 loss:  1.7079368829727173 mini-batch correct:  42.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  224 loss:  1.5603275299072266 mini-batch correct:  55.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  225 loss:  1.677714228630066 mini-batch correct:  47.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  226 loss:  1.6800410747528076 mini-batch correct:  45.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  227 loss:  1.5751594305038452 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  228 loss:  1.5879626274108887 mini-batch correct:  50.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  229 loss:  1.6261377334594727 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  230 loss:  1.4899762868881226 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  231 loss:  1.6032381057739258 mini-batch correct:  58.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  232 loss:  1.7617489099502563 mini-batch correct:  46.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  233 loss:  1.6121010780334473 mini-batch correct:  47.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  234 loss:  1.6207513809204102 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  235 loss:  1.6494572162628174 mini-batch correct:  50.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  236 loss:  1.5400927066802979 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  237 loss:  1.5285015106201172 mini-batch correct:  55.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  238 loss:  1.598874807357788 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  239 loss:  1.7084099054336548 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  240 loss:  1.5499135255813599 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  241 loss:  1.4942457675933838 mini-batch correct:  59.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  242 loss:  1.525604009628296 mini-batch correct:  59.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  243 loss:  1.7295604944229126 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  244 loss:  1.5630251169204712 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  245 loss:  1.7730077505111694 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  246 loss:  1.4763593673706055 mini-batch correct:  66.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  247 loss:  1.4815504550933838 mini-batch correct:  58.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  248 loss:  1.7665836811065674 mini-batch correct:  44.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  249 loss:  1.6875611543655396 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  250 loss:  1.5294550657272339 mini-batch correct:  60.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  251 loss:  1.6267874240875244 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  252 loss:  1.6802456378936768 mini-batch correct:  46.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  253 loss:  1.5214402675628662 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  254 loss:  1.5753731727600098 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  255 loss:  1.614835262298584 mini-batch correct:  45.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  256 loss:  1.5370266437530518 mini-batch correct:  59.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  257 loss:  1.5976771116256714 mini-batch correct:  46.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  258 loss:  1.5121219158172607 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  259 loss:  1.4867603778839111 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  260 loss:  1.5910617113113403 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  261 loss:  1.4387425184249878 mini-batch correct:  60.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  262 loss:  1.78400456905365 mini-batch correct:  46.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  263 loss:  1.619320034980774 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  264 loss:  1.5815601348876953 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  265 loss:  1.5548388957977295 mini-batch correct:  50.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  266 loss:  1.6263102293014526 mini-batch correct:  59.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  267 loss:  1.638423204421997 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  268 loss:  1.5209660530090332 mini-batch correct:  58.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  269 loss:  1.5654292106628418 mini-batch correct:  49.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  270 loss:  1.5506104230880737 mini-batch correct:  55.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  271 loss:  1.5824211835861206 mini-batch correct:  55.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  272 loss:  1.7058446407318115 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  273 loss:  1.5865824222564697 mini-batch correct:  58.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  274 loss:  1.7095891237258911 mini-batch correct:  47.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  275 loss:  1.4842109680175781 mini-batch correct:  59.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  276 loss:  1.6485048532485962 mini-batch correct:  58.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  277 loss:  1.5298491716384888 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  278 loss:  1.5715227127075195 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  279 loss:  1.4984996318817139 mini-batch correct:  58.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  280 loss:  1.5709339380264282 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  281 loss:  1.5650153160095215 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  282 loss:  1.5485135316848755 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  283 loss:  1.7293223142623901 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  284 loss:  1.4749798774719238 mini-batch correct:  58.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  285 loss:  1.4396189451217651 mini-batch correct:  61.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  286 loss:  1.721919298171997 mini-batch correct:  48.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  287 loss:  1.571602702140808 mini-batch correct:  58.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  288 loss:  1.6258788108825684 mini-batch correct:  62.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  289 loss:  1.555246353149414 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  290 loss:  1.6633573770523071 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  291 loss:  1.7369886636734009 mini-batch correct:  46.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  292 loss:  1.57488214969635 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  293 loss:  1.7061806917190552 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  294 loss:  1.6568212509155273 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  295 loss:  1.4996092319488525 mini-batch correct:  60.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  296 loss:  1.4819070100784302 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  297 loss:  1.4490382671356201 mini-batch correct:  58.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  298 loss:  1.5788451433181763 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  299 loss:  1.6986225843429565 mini-batch correct:  47.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  300 loss:  1.6286025047302246 mini-batch correct:  60.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  301 loss:  1.6619832515716553 mini-batch correct:  55.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  302 loss:  1.328520655632019 mini-batch correct:  72.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  303 loss:  1.5612223148345947 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  304 loss:  1.43800687789917 mini-batch correct:  63.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  305 loss:  1.5598691701889038 mini-batch correct:  55.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  306 loss:  1.5634915828704834 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  307 loss:  1.5668179988861084 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  308 loss:  1.6371128559112549 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  309 loss:  1.4638416767120361 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  310 loss:  1.5254489183425903 mini-batch correct:  62.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  311 loss:  1.5127273797988892 mini-batch correct:  64.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  312 loss:  1.537309169769287 mini-batch correct:  55.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  313 loss:  1.7391947507858276 mini-batch correct:  48.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  314 loss:  1.6418734788894653 mini-batch correct:  58.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  315 loss:  1.4015114307403564 mini-batch correct:  59.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  316 loss:  1.3753876686096191 mini-batch correct:  59.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  317 loss:  1.5696589946746826 mini-batch correct:  48.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  318 loss:  1.630745768547058 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  319 loss:  1.7867953777313232 mini-batch correct:  46.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  320 loss:  1.5224707126617432 mini-batch correct:  58.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  321 loss:  1.4674010276794434 mini-batch correct:  63.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  322 loss:  1.6139274835586548 mini-batch correct:  50.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  323 loss:  1.6796196699142456 mini-batch correct:  48.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  324 loss:  1.5252999067306519 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  325 loss:  1.555954098701477 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  326 loss:  1.5524057149887085 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  327 loss:  1.5303466320037842 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  328 loss:  1.593174695968628 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  329 loss:  1.475420594215393 mini-batch correct:  61.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  330 loss:  1.5012753009796143 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  331 loss:  1.6396441459655762 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  332 loss:  1.8038023710250854 mini-batch correct:  44.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  333 loss:  1.4927438497543335 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  334 loss:  1.6456266641616821 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  335 loss:  1.6184133291244507 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  336 loss:  1.4892958402633667 mini-batch correct:  55.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  337 loss:  1.4449797868728638 mini-batch correct:  68.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  338 loss:  1.5370090007781982 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  339 loss:  1.5702468156814575 mini-batch correct:  49.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  340 loss:  1.7352280616760254 mini-batch correct:  49.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  341 loss:  1.5990080833435059 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  342 loss:  1.5455576181411743 mini-batch correct:  55.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  343 loss:  1.4848774671554565 mini-batch correct:  62.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  344 loss:  1.3962526321411133 mini-batch correct:  64.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  345 loss:  1.6199487447738647 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  346 loss:  1.6183414459228516 mini-batch correct:  51.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  347 loss:  1.494551658630371 mini-batch correct:  58.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  348 loss:  1.568604588508606 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  349 loss:  1.5573493242263794 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  350 loss:  1.5412517786026 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  351 loss:  1.6453166007995605 mini-batch correct:  65.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  352 loss:  1.5700486898422241 mini-batch correct:  60.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  353 loss:  1.585343837738037 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  354 loss:  1.4582847356796265 mini-batch correct:  68.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  355 loss:  1.5023698806762695 mini-batch correct:  56.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  356 loss:  1.816443920135498 mini-batch correct:  44.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  357 loss:  1.6694976091384888 mini-batch correct:  55.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  358 loss:  1.6964064836502075 mini-batch correct:  48.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  359 loss:  1.5832877159118652 mini-batch correct:  55.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  360 loss:  1.5806562900543213 mini-batch correct:  65.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  361 loss:  1.74580717086792 mini-batch correct:  42.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  362 loss:  1.3987514972686768 mini-batch correct:  60.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  363 loss:  1.5671707391738892 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  364 loss:  1.6094343662261963 mini-batch correct:  62.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  365 loss:  1.7868105173110962 mini-batch correct:  46.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  366 loss:  1.5416817665100098 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  367 loss:  1.7065787315368652 mini-batch correct:  48.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  368 loss:  1.7881437540054321 mini-batch correct:  45.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  369 loss:  1.5472451448440552 mini-batch correct:  50.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  370 loss:  1.5695974826812744 mini-batch correct:  64.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  371 loss:  1.6551579236984253 mini-batch correct:  62.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  372 loss:  1.4089465141296387 mini-batch correct:  65.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  373 loss:  1.5514616966247559 mini-batch correct:  60.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  374 loss:  1.4899870157241821 mini-batch correct:  57.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  375 loss:  1.4201945066452026 mini-batch correct:  61.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  376 loss:  1.5859148502349854 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  377 loss:  1.4934043884277344 mini-batch correct:  65.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  378 loss:  1.6357933282852173 mini-batch correct:  53.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  379 loss:  1.4428064823150635 mini-batch correct:  62.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  380 loss:  1.5707367658615112 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  381 loss:  1.447237491607666 mini-batch correct:  54.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  382 loss:  1.651775598526001 mini-batch correct:  52.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  383 loss:  1.7041693925857544 mini-batch correct:  45.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  384 loss:  1.5122263431549072 mini-batch correct:  55.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  385 loss:  1.5931422710418701 mini-batch correct:  59.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  386 loss:  1.6267056465148926 mini-batch correct:  62.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  387 loss:  1.6948587894439697 mini-batch correct:  55.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  388 loss:  1.545257568359375 mini-batch correct:  55.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  389 loss:  1.4061921834945679 mini-batch correct:  67.0 train lr: 0.008100000000000001\n",
      "epoch:  2 train step:  390 loss:  1.678997278213501 mini-batch correct:  33.0 train lr: 0.008100000000000001\n",
      "epoch: 2 loss:  1.60141727535785 test correct:  57.18987341772152\n",
      "epoch is:  3\n",
      "epoch:  3 train step:  0 loss:  1.7580441236495972 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  1 loss:  1.5440425872802734 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  2 loss:  1.3724744319915771 mini-batch correct:  64.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  3 loss:  1.5548213720321655 mini-batch correct:  50.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  4 loss:  1.4873003959655762 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  5 loss:  1.6908080577850342 mini-batch correct:  51.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  6 loss:  1.5904771089553833 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  7 loss:  1.5983226299285889 mini-batch correct:  56.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  8 loss:  1.6800342798233032 mini-batch correct:  50.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  9 loss:  1.613978385925293 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  10 loss:  1.5043237209320068 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  11 loss:  1.6000334024429321 mini-batch correct:  54.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  12 loss:  1.617651343345642 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  13 loss:  1.4222378730773926 mini-batch correct:  54.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  14 loss:  1.4685970544815063 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  15 loss:  1.386614203453064 mini-batch correct:  70.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  16 loss:  1.5176897048950195 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  17 loss:  1.5153478384017944 mini-batch correct:  56.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  18 loss:  1.603069543838501 mini-batch correct:  55.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  19 loss:  1.5967317819595337 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  20 loss:  1.3973532915115356 mini-batch correct:  67.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  21 loss:  1.393774151802063 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  22 loss:  1.4879611730575562 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  23 loss:  1.4473114013671875 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  24 loss:  1.6644811630249023 mini-batch correct:  42.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  25 loss:  1.3993934392929077 mini-batch correct:  66.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  26 loss:  1.4765145778656006 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  27 loss:  1.455016016960144 mini-batch correct:  51.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  28 loss:  1.6005195379257202 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  29 loss:  1.5245364904403687 mini-batch correct:  49.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  30 loss:  1.4988576173782349 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  31 loss:  1.5614688396453857 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  32 loss:  1.3460345268249512 mini-batch correct:  71.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  33 loss:  1.4147542715072632 mini-batch correct:  64.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  34 loss:  1.4648953676223755 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  35 loss:  1.6245096921920776 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  36 loss:  1.6243869066238403 mini-batch correct:  51.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  37 loss:  1.454026699066162 mini-batch correct:  68.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  38 loss:  1.5747064352035522 mini-batch correct:  49.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  39 loss:  1.3841044902801514 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  40 loss:  1.4212013483047485 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  41 loss:  1.4986592531204224 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  42 loss:  1.6104291677474976 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  43 loss:  1.4635659456253052 mini-batch correct:  66.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  44 loss:  1.531761884689331 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  45 loss:  1.536157488822937 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  46 loss:  1.3948662281036377 mini-batch correct:  65.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  47 loss:  1.441392421722412 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  48 loss:  1.5069022178649902 mini-batch correct:  56.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  49 loss:  1.5340298414230347 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  50 loss:  1.5654019117355347 mini-batch correct:  56.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  51 loss:  1.5630770921707153 mini-batch correct:  54.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  52 loss:  1.6538113355636597 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  53 loss:  1.6105157136917114 mini-batch correct:  55.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  54 loss:  1.4991824626922607 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  55 loss:  1.6170564889907837 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  56 loss:  1.5414766073226929 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  57 loss:  1.5836604833602905 mini-batch correct:  47.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  58 loss:  1.5045114755630493 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  59 loss:  1.4757134914398193 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  60 loss:  1.3756871223449707 mini-batch correct:  65.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  61 loss:  1.530426263809204 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  62 loss:  1.6684250831604004 mini-batch correct:  52.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  63 loss:  1.4517128467559814 mini-batch correct:  67.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  64 loss:  1.548580288887024 mini-batch correct:  52.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  65 loss:  1.6196919679641724 mini-batch correct:  55.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  66 loss:  1.4982508420944214 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  67 loss:  1.3832283020019531 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  68 loss:  1.525240421295166 mini-batch correct:  55.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  69 loss:  1.7220427989959717 mini-batch correct:  51.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  70 loss:  1.4193415641784668 mini-batch correct:  67.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  71 loss:  1.5946859121322632 mini-batch correct:  54.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  72 loss:  1.6679160594940186 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  73 loss:  1.5390238761901855 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  74 loss:  1.5007928609848022 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  75 loss:  1.5088133811950684 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  76 loss:  1.5854475498199463 mini-batch correct:  52.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  77 loss:  1.4031503200531006 mini-batch correct:  64.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  78 loss:  1.4957566261291504 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  79 loss:  1.6319890022277832 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  80 loss:  1.4902853965759277 mini-batch correct:  64.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  81 loss:  1.5722557306289673 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  82 loss:  1.4745111465454102 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  83 loss:  1.5066547393798828 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  84 loss:  1.5522139072418213 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  85 loss:  1.5206338167190552 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  86 loss:  1.5538145303726196 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  87 loss:  1.6483852863311768 mini-batch correct:  49.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  88 loss:  1.5281697511672974 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  89 loss:  1.514668583869934 mini-batch correct:  56.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  90 loss:  1.597470760345459 mini-batch correct:  48.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  91 loss:  1.4634075164794922 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  92 loss:  1.537194013595581 mini-batch correct:  55.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  93 loss:  1.3834465742111206 mini-batch correct:  68.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  94 loss:  1.524878978729248 mini-batch correct:  67.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  95 loss:  1.5465435981750488 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  96 loss:  1.4823110103607178 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  97 loss:  1.5507460832595825 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  98 loss:  1.5855103731155396 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  99 loss:  1.5752686262130737 mini-batch correct:  55.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  100 loss:  1.6153571605682373 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  101 loss:  1.5092395544052124 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  102 loss:  1.6881513595581055 mini-batch correct:  52.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  103 loss:  1.5749025344848633 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  104 loss:  1.4874004125595093 mini-batch correct:  54.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  105 loss:  1.5594508647918701 mini-batch correct:  52.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  106 loss:  1.663771390914917 mini-batch correct:  54.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  107 loss:  1.5634863376617432 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  108 loss:  1.570546269416809 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  109 loss:  1.493330478668213 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  110 loss:  1.632236123085022 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  111 loss:  1.6104822158813477 mini-batch correct:  52.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  112 loss:  1.438494086265564 mini-batch correct:  66.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  113 loss:  1.5259130001068115 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  114 loss:  1.5882899761199951 mini-batch correct:  49.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  115 loss:  1.3991038799285889 mini-batch correct:  64.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  116 loss:  1.6166791915893555 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  117 loss:  1.5208520889282227 mini-batch correct:  49.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  118 loss:  1.5439642667770386 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  119 loss:  1.4514350891113281 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  120 loss:  1.6121197938919067 mini-batch correct:  51.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  121 loss:  1.516144037246704 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  122 loss:  1.5741828680038452 mini-batch correct:  54.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  123 loss:  1.4120376110076904 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  124 loss:  1.511536717414856 mini-batch correct:  64.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  125 loss:  1.6468833684921265 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  126 loss:  1.4905693531036377 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  127 loss:  1.4245405197143555 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  128 loss:  1.5736857652664185 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  129 loss:  1.5677663087844849 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  130 loss:  1.497804045677185 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  131 loss:  1.5976802110671997 mini-batch correct:  49.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  132 loss:  1.5147289037704468 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  133 loss:  1.4355080127716064 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  134 loss:  1.5789318084716797 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  135 loss:  1.4570947885513306 mini-batch correct:  65.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  136 loss:  1.552974820137024 mini-batch correct:  56.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  137 loss:  1.4828295707702637 mini-batch correct:  64.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  138 loss:  1.519260287284851 mini-batch correct:  54.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  139 loss:  1.5889779329299927 mini-batch correct:  49.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  140 loss:  1.6212892532348633 mini-batch correct:  48.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  141 loss:  1.3406049013137817 mini-batch correct:  70.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  142 loss:  1.6446104049682617 mini-batch correct:  52.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  143 loss:  1.5884913206100464 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  144 loss:  1.6168972253799438 mini-batch correct:  51.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  145 loss:  1.4719754457473755 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  146 loss:  1.6429394483566284 mini-batch correct:  50.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  147 loss:  1.6007540225982666 mini-batch correct:  55.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  148 loss:  1.6791563034057617 mini-batch correct:  54.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  149 loss:  1.644261121749878 mini-batch correct:  47.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  150 loss:  1.565946340560913 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  151 loss:  1.592869758605957 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  152 loss:  1.5724701881408691 mini-batch correct:  54.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  153 loss:  1.5403790473937988 mini-batch correct:  51.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  154 loss:  1.6951555013656616 mini-batch correct:  49.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  155 loss:  1.6546423435211182 mini-batch correct:  51.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  156 loss:  1.587958812713623 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  157 loss:  1.4182014465332031 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  158 loss:  1.4996321201324463 mini-batch correct:  55.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  159 loss:  1.5495052337646484 mini-batch correct:  52.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  160 loss:  1.4929771423339844 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  161 loss:  1.40204656124115 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  162 loss:  1.499372959136963 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  163 loss:  1.5218573808670044 mini-batch correct:  52.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  164 loss:  1.4950095415115356 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  165 loss:  1.4878908395767212 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  166 loss:  1.3931330442428589 mini-batch correct:  65.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  167 loss:  1.4978214502334595 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  168 loss:  1.4695543050765991 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  169 loss:  1.6383627653121948 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  170 loss:  1.3699796199798584 mini-batch correct:  66.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  171 loss:  1.4625447988510132 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  172 loss:  1.4869017601013184 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  173 loss:  1.5410388708114624 mini-batch correct:  56.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  174 loss:  1.3474035263061523 mini-batch correct:  65.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  175 loss:  1.6096954345703125 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  176 loss:  1.525956392288208 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  177 loss:  1.5234646797180176 mini-batch correct:  52.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  178 loss:  1.5174436569213867 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  179 loss:  1.4946378469467163 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  180 loss:  1.4871220588684082 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  181 loss:  1.4683300256729126 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  182 loss:  1.6394628286361694 mini-batch correct:  50.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  183 loss:  1.354356288909912 mini-batch correct:  76.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  184 loss:  1.5346827507019043 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  185 loss:  1.3511040210723877 mini-batch correct:  71.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  186 loss:  1.46379554271698 mini-batch correct:  55.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  187 loss:  1.3836725950241089 mini-batch correct:  64.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  188 loss:  1.4037498235702515 mini-batch correct:  71.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  189 loss:  1.5496423244476318 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  190 loss:  1.7010128498077393 mini-batch correct:  48.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  191 loss:  1.4962016344070435 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  192 loss:  1.426658034324646 mini-batch correct:  72.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  193 loss:  1.5250991582870483 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  194 loss:  1.4621883630752563 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  195 loss:  1.4648734331130981 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  196 loss:  1.564176082611084 mini-batch correct:  56.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  197 loss:  1.4867371320724487 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  198 loss:  1.47048020362854 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  199 loss:  1.5948446989059448 mini-batch correct:  55.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  200 loss:  1.51698637008667 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  201 loss:  1.608681321144104 mini-batch correct:  50.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  202 loss:  1.643532156944275 mini-batch correct:  48.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  203 loss:  1.5089503526687622 mini-batch correct:  52.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  204 loss:  1.4161690473556519 mini-batch correct:  65.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  205 loss:  1.4374449253082275 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  206 loss:  1.5246347188949585 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  207 loss:  1.4568767547607422 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  208 loss:  1.5370771884918213 mini-batch correct:  55.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  209 loss:  1.4337573051452637 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  210 loss:  1.521031141281128 mini-batch correct:  52.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  211 loss:  1.4937227964401245 mini-batch correct:  56.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  212 loss:  1.4921410083770752 mini-batch correct:  54.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  213 loss:  1.6047643423080444 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  214 loss:  1.5222489833831787 mini-batch correct:  54.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  215 loss:  1.6254212856292725 mini-batch correct:  54.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  216 loss:  1.3540071249008179 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  217 loss:  1.6183528900146484 mini-batch correct:  49.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  218 loss:  1.5963337421417236 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  219 loss:  1.607854962348938 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  220 loss:  1.4260975122451782 mini-batch correct:  64.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  221 loss:  1.5163912773132324 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  222 loss:  1.6533010005950928 mini-batch correct:  51.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  223 loss:  1.4447147846221924 mini-batch correct:  67.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  224 loss:  1.274536371231079 mini-batch correct:  68.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  225 loss:  1.42826247215271 mini-batch correct:  56.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  226 loss:  1.425559401512146 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  227 loss:  1.6306973695755005 mini-batch correct:  54.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  228 loss:  1.3117833137512207 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  229 loss:  1.4107840061187744 mini-batch correct:  68.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  230 loss:  1.4407448768615723 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  231 loss:  1.5304282903671265 mini-batch correct:  54.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  232 loss:  1.4278661012649536 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  233 loss:  1.419366717338562 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  234 loss:  1.476122260093689 mini-batch correct:  54.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  235 loss:  1.4664002656936646 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  236 loss:  1.4722613096237183 mini-batch correct:  55.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  237 loss:  1.4156312942504883 mini-batch correct:  66.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  238 loss:  1.464574933052063 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  239 loss:  1.4944320917129517 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  240 loss:  1.4988322257995605 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  241 loss:  1.433557152748108 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  242 loss:  1.4415900707244873 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  243 loss:  1.5337527990341187 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  244 loss:  1.64238440990448 mini-batch correct:  50.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  245 loss:  1.4474388360977173 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  246 loss:  1.559214472770691 mini-batch correct:  64.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  247 loss:  1.507454752922058 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  248 loss:  1.6015260219573975 mini-batch correct:  51.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  249 loss:  1.4197580814361572 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  250 loss:  1.3428987264633179 mini-batch correct:  64.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  251 loss:  1.4405033588409424 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  252 loss:  1.4233357906341553 mini-batch correct:  68.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  253 loss:  1.4852583408355713 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  254 loss:  1.4067496061325073 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  255 loss:  1.560512661933899 mini-batch correct:  51.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  256 loss:  1.3486074209213257 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  257 loss:  1.4067230224609375 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  258 loss:  1.3546111583709717 mini-batch correct:  67.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  259 loss:  1.3551976680755615 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  260 loss:  1.4266682863235474 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  261 loss:  1.497936487197876 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  262 loss:  1.5216628313064575 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  263 loss:  1.3608726263046265 mini-batch correct:  69.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  264 loss:  1.3340519666671753 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  265 loss:  1.671093225479126 mini-batch correct:  56.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  266 loss:  1.644141435623169 mini-batch correct:  50.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  267 loss:  1.4187053442001343 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  268 loss:  1.43631112575531 mini-batch correct:  65.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  269 loss:  1.3680499792099 mini-batch correct:  67.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  270 loss:  1.3893404006958008 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  271 loss:  1.4788919687271118 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  272 loss:  1.4722740650177002 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  273 loss:  1.5329538583755493 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  274 loss:  1.7629830837249756 mini-batch correct:  50.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  275 loss:  1.504146695137024 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  276 loss:  1.3730754852294922 mini-batch correct:  65.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  277 loss:  1.3472440242767334 mini-batch correct:  70.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  278 loss:  1.487486720085144 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  279 loss:  1.3932465314865112 mini-batch correct:  65.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  280 loss:  1.5519020557403564 mini-batch correct:  50.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  281 loss:  1.4366905689239502 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  282 loss:  1.4578930139541626 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  283 loss:  1.5203124284744263 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  284 loss:  1.466638207435608 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  285 loss:  1.6216905117034912 mini-batch correct:  52.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  286 loss:  1.5231267213821411 mini-batch correct:  56.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  287 loss:  1.4428890943527222 mini-batch correct:  64.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  288 loss:  1.487159013748169 mini-batch correct:  65.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  289 loss:  1.4780982732772827 mini-batch correct:  67.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  290 loss:  1.546605110168457 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  291 loss:  1.4794310331344604 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  292 loss:  1.6361875534057617 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  293 loss:  1.6453523635864258 mini-batch correct:  55.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  294 loss:  1.368399977684021 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  295 loss:  1.3607248067855835 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  296 loss:  1.4125745296478271 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  297 loss:  1.4346600770950317 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  298 loss:  1.4999721050262451 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  299 loss:  1.3466212749481201 mini-batch correct:  74.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  300 loss:  1.608031153678894 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  301 loss:  1.6163640022277832 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  302 loss:  1.4969232082366943 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  303 loss:  1.5261883735656738 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  304 loss:  1.3920353651046753 mini-batch correct:  56.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  305 loss:  1.5706682205200195 mini-batch correct:  51.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  306 loss:  1.636534571647644 mini-batch correct:  46.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  307 loss:  1.3933968544006348 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  308 loss:  1.5141239166259766 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  309 loss:  1.4875632524490356 mini-batch correct:  66.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  310 loss:  1.2901406288146973 mini-batch correct:  71.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  311 loss:  1.408008098602295 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  312 loss:  1.5459638833999634 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  313 loss:  1.3513538837432861 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  314 loss:  1.3705482482910156 mini-batch correct:  65.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  315 loss:  1.5361082553863525 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  316 loss:  1.3769898414611816 mini-batch correct:  65.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  317 loss:  1.2974731922149658 mini-batch correct:  67.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  318 loss:  1.438709020614624 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  319 loss:  1.455074429512024 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  320 loss:  1.467246651649475 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  321 loss:  1.4588805437088013 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  322 loss:  1.493179202079773 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  323 loss:  1.4118164777755737 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  324 loss:  1.4782675504684448 mini-batch correct:  52.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  325 loss:  1.352239727973938 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  326 loss:  1.533954381942749 mini-batch correct:  56.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  327 loss:  1.4416732788085938 mini-batch correct:  66.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  328 loss:  1.3962633609771729 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  329 loss:  1.4497957229614258 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  330 loss:  1.6557130813598633 mini-batch correct:  48.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  331 loss:  1.3859837055206299 mini-batch correct:  55.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  332 loss:  1.452370524406433 mini-batch correct:  56.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  333 loss:  1.4117261171340942 mini-batch correct:  67.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  334 loss:  1.4299975633621216 mini-batch correct:  64.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  335 loss:  1.4067490100860596 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  336 loss:  1.2864041328430176 mini-batch correct:  73.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  337 loss:  1.4922759532928467 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  338 loss:  1.5198419094085693 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  339 loss:  1.3868826627731323 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  340 loss:  1.5270640850067139 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  341 loss:  1.4551138877868652 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  342 loss:  1.2784051895141602 mini-batch correct:  67.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  343 loss:  1.4746707677841187 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  344 loss:  1.3036725521087646 mini-batch correct:  72.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  345 loss:  1.4068115949630737 mini-batch correct:  66.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  346 loss:  1.6317965984344482 mini-batch correct:  51.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  347 loss:  1.440443992614746 mini-batch correct:  66.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  348 loss:  1.5570985078811646 mini-batch correct:  51.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  349 loss:  1.3411823511123657 mini-batch correct:  64.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  350 loss:  1.539936900138855 mini-batch correct:  56.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  351 loss:  1.2456202507019043 mini-batch correct:  68.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  352 loss:  1.3937901258468628 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  353 loss:  1.3862366676330566 mini-batch correct:  62.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  354 loss:  1.4526876211166382 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  355 loss:  1.576547622680664 mini-batch correct:  53.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  356 loss:  1.6825413703918457 mini-batch correct:  40.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  357 loss:  1.5430837869644165 mini-batch correct:  56.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  358 loss:  1.4171684980392456 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  359 loss:  1.5546332597732544 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  360 loss:  1.5193670988082886 mini-batch correct:  56.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  361 loss:  1.5034643411636353 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  362 loss:  1.3854461908340454 mini-batch correct:  69.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  363 loss:  1.5478085279464722 mini-batch correct:  54.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  364 loss:  1.373600721359253 mini-batch correct:  65.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  365 loss:  1.4429231882095337 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  366 loss:  1.5993280410766602 mini-batch correct:  52.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  367 loss:  1.4939818382263184 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  368 loss:  1.5025572776794434 mini-batch correct:  52.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  369 loss:  1.448258876800537 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  370 loss:  1.475932240486145 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  371 loss:  1.49062979221344 mini-batch correct:  54.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  372 loss:  1.563592553138733 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  373 loss:  1.516055941581726 mini-batch correct:  55.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  374 loss:  1.52506422996521 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  375 loss:  1.3483749628067017 mini-batch correct:  63.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  376 loss:  1.4012954235076904 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  377 loss:  1.3837110996246338 mini-batch correct:  69.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  378 loss:  1.4081940650939941 mini-batch correct:  59.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  379 loss:  1.3395378589630127 mini-batch correct:  70.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  380 loss:  1.443716287612915 mini-batch correct:  64.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  381 loss:  1.4968622922897339 mini-batch correct:  58.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  382 loss:  1.6408891677856445 mini-batch correct:  57.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  383 loss:  1.3702161312103271 mini-batch correct:  65.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  384 loss:  1.4581972360610962 mini-batch correct:  56.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  385 loss:  1.4718437194824219 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  386 loss:  1.407467007637024 mini-batch correct:  60.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  387 loss:  1.4209576845169067 mini-batch correct:  66.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  388 loss:  1.3418887853622437 mini-batch correct:  61.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  389 loss:  1.3416213989257812 mini-batch correct:  65.0 train lr: 0.007290000000000001\n",
      "epoch:  3 train step:  390 loss:  1.6010220050811768 mini-batch correct:  36.0 train lr: 0.007290000000000001\n",
      "epoch: 3 loss:  1.4533780722678462 test correct:  62.0253164556962\n",
      "epoch is:  4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-505238a55c5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'timeit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\nif not os.path.exists('log'):\\n    os.mkdir('log')\\nwriter = tensorboardX.SummaryWriter('log')\\nstep_n = 0\\n\\n\\nfor epoch in range(num_epoch):\\n    print('epoch is: ', epoch)  \\n    net.train()  # train BN dropout\\n    \\n    for i, data in enumerate(train_loader):\\n        net.train()  # train BN dropout\\n        inputs, labels = data\\n        inputs, labels = inputs.to(device), labels.to(device)        \\n\\n        outputs = net(inputs)\\n        loss = loss_func(outputs, labels)\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()        \\n        \\n        _, pred = torch.max(outputs, dim=1)\\n        correct = pred.eq(labels.data).cpu().sum()\\n\\n#         print('train lr:', optimizer.state_dict()['param_groups'][0]['lr'])    \\n        print('epoch: ', epoch, 'train step: ', i, 'loss: ', loss.item(),\\\\\\n             'mini-batch correct: ', 100.0 * correct.item() / batch_size,\\\\\\n             'train lr:', optimizer.state_dict()['param_groups'][0]['lr'])\\n        \\n        writer.add_scalar('train loss', loss.item(), global_step=step_n)\\n        writer.add_scalar('train correct', 100.0 * correct.item() / batch_size, global_step=step_n)\\n        \\n        im = torchvision.utils.make_grid(inputs)\\n        writer.add_image('train im', im, global_step=step_n)\\n        \\n        step_n += 1\\n        \\n    if not os.path.exists('model'):\\n        os.mkdir('model')\\n    torch.save(net.state_dict(), 'model/{}.pth'.format(epoch))\\n    scheduler.step()\\n\\n    \\n    sum_loss = 0\\n    sum_correct = 0    \\n    for i, data in enumerate(test_loader):\\n        net.eval()  \\n        inputs, labels = data\\n        inputs, labels = inputs.to(device), labels.to(device)        \\n\\n        outputs = net(inputs)\\n        loss = loss_func(outputs, labels)        \\n        _, pred = torch.max(outputs, dim=1)\\n        correct = pred.eq(labels.data).cpu().sum()\\n        \\n        sum_loss += loss.item()\\n        sum_correct += correct.item()\\n        \\n        im = torchvision.utils.make_grid(inputs)\\n        writer.add_image('test im', im, global_step=step_n)\\n    \\n    test_loss = sum_loss * 1.0 / len(test_loader)\\n    test_correct = sum_correct * 100.0 / len(test_loader) / batch_size\\n    writer.add_scalar('test loss', loss.item(), global_step=epoch+1)\\n    writer.add_scalar('test correct', 100.0 * correct.item() / batch_size, global_step=epoch+1)\\n    print('epoch:', epoch, 'loss: ', test_loss,\\\\\\n         'test correct: ', test_correct)\\n        \\nwriter.close()\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2360\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2361\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2362\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2363\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<C:\\ProgramData\\Anaconda3\\lib\\site-packages\\decorator.py:decorator-gen-61>\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1158\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1160\u001b[1;33m                 \u001b[0mtime_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1161\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             \u001b[0mtiming\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    289\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    735\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "if not os.path.exists('{}log'.format(PATH_LOG)):\n",
    "    os.mkdir('{}log'.format(PATH_LOG))\n",
    "writer = tensorboardX.SummaryWriter('{}log'.format(PATH_LOG))\n",
    "step_n = 0\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    print('epoch is: ', epoch)  \n",
    "    net.train()  # train BN dropout\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        net.train()  # train BN dropout\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)        \n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        \n",
    "        _, pred = torch.max(outputs, dim=1)\n",
    "        correct = pred.eq(labels.data).cpu().sum()\n",
    "\n",
    "#         print('train lr:', optimizer.state_dict()['param_groups'][0]['lr'])    \n",
    "        print('epoch: ', epoch, 'train step: ', i, 'loss: ', loss.item(),\\\n",
    "             'mini-batch correct: ', 100.0 * correct.item() / batch_size,\\\n",
    "             'train lr:', optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "        \n",
    "        writer.add_scalar('train loss', loss.item(), global_step=step_n)\n",
    "        writer.add_scalar('train correct', 100.0 * correct.item() / batch_size, global_step=step_n)\n",
    "        \n",
    "        im = torchvision.utils.make_grid(inputs)\n",
    "        writer.add_image('train im', im, global_step=step_n)\n",
    "        \n",
    "        step_n += 1\n",
    "        \n",
    "    if not os.path.exists('{}model'.format(PATH_LOG)):\n",
    "        os.mkdir('{}model'.format(PATH_LOG))\n",
    "    torch.save(net.state_dict(), 'model/{}.pth'.format(epoch))\n",
    "    scheduler.step()\n",
    "\n",
    "    \n",
    "    sum_loss = 0\n",
    "    sum_correct = 0    \n",
    "    for i, data in enumerate(test_loader):\n",
    "        net.eval()  \n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)        \n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_func(outputs, labels)        \n",
    "        _, pred = torch.max(outputs, dim=1)\n",
    "        correct = pred.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        sum_loss += loss.item()\n",
    "        sum_correct += correct.item()\n",
    "        \n",
    "        im = torchvision.utils.make_grid(inputs)\n",
    "        writer.add_image('test im', im, global_step=step_n)\n",
    "    \n",
    "    test_loss = sum_loss * 1.0 / len(test_loader)\n",
    "    test_correct = sum_correct * 100.0 / len(test_loader) / batch_size\n",
    "    writer.add_scalar('test loss', loss.item(), global_step=epoch+1)\n",
    "    writer.add_scalar('test correct', 100.0 * correct.item() / batch_size, global_step=epoch+1)\n",
    "    print('epoch:', epoch, 'loss: ', test_loss,\\\n",
    "         'test correct: ', test_correct)\n",
    "        \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
